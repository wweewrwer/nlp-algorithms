{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 79170 sentences.\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "print(\"Reading CSV file...\")\n",
    "with open('reddit-comments-2015-08.csv', 'rt', encoding= 'utf-8') as f:\n",
    "    reader = csv.reader(f, skipinitialspace = True)\n",
    "    next(reader)\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print(\"Parsed %d sentences.\" % (len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65441 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Found %d unique words tokens.\" % len(list(word_freq.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'crank' and appeared 10 times.\n"
     ]
    }
   ],
   "source": [
    "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "print(\"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print(\"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START what are n't you understanding about this ? !\n",
      "[0, 51, 27, 16, 10, 858, 54, 25, 34, 69]\n",
      "\n",
      "y:\n",
      "what are n't you understanding about this ? ! SENTENCE_END\n",
      "[51, 27, 16, 10, 858, 54, 25, 34, 69, 1]\n"
     ]
    }
   ],
   "source": [
    "x_example, y_example = x_train[17], y_train[17]\n",
    "print(\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
    "print(\"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        self.U = torch.randn((hidden_dim, word_dim))\n",
    "        self.U.data = self.U.data.uniform_(-np.sqrt(1./word_dim), np.sqrt(1./word_dim))\n",
    "        self.V = torch.randn((word_dim, hidden_dim))\n",
    "        self.V.data = self.V.data.uniform_(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim))\n",
    "        self.W = torch.randn((hidden_dim, hidden_dim))\n",
    "        self.W.data = self.W.data.uniform_(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim))\n",
    "        self.U.requires_grad=True\n",
    "        self.V.requires_grad=True\n",
    "        self.W.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    xt = torch.exp(x - torch.max(x))\n",
    "    return xt / torch.sum(xt)\n",
    "\n",
    "def forward_propagation(self, x):\n",
    "    T = len(x)\n",
    "    s = torch.zeros(self.hidden_dim)\n",
    "    o = torch.zeros((T, self.word_dim))\n",
    "    for t in np.arange(T):\n",
    "        s = torch.tanh(self.U[:,x[t]] + self.W.mv(s))\n",
    "        o[t] = softmax(self.V.mv(s))\n",
    "    return o\n",
    "    \n",
    "RNNNumpy.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    o = self.forward_propagation(x)\n",
    "    return torch.argmax(o, dim=1)\n",
    "\n",
    "RNNNumpy.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45, 8000])\n",
      "tensor([[0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001],\n",
      "        [0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001],\n",
      "        [0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001],\n",
      "        ...,\n",
      "        [0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001],\n",
      "        [0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001],\n",
      "        [0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001]],\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o = model.forward_propagation(x_train[10])\n",
    "print(o.shape)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45])\n",
      "tensor([4781, 4695, 3294, 2528, 1377, 7649, 6863,    8, 2503,  980, 1976, 4642,\n",
      "        2831, 5587, 5348, 1114, 3015, 4398, 2932, 2915, 4432, 1254, 1136, 1199,\n",
      "        4398,  427, 5581, 1986, 1633, 2097, 1224, 6235, 4935, 4602,  130,  769,\n",
      "        4398, 4159, 6561, 7486, 5768, 5360, 1762, 7087, 3755])\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_train[10])\n",
    "print(predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    \n",
    "    for i in np.arange(len(y)):\n",
    "        o = self.forward_propagation(x[i])\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        L = L + -1 * torch.sum(torch.log(correct_word_predictions))\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(self, x, y):\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 8.987197\n",
      "Actual loss: 8.986947\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print(\"Actual loss: %f\" % model.calculate_loss(x_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_sgd(model, x_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        loss = model.calculate_loss(x_train, y_train)\n",
    "        loss.backward()\n",
    "        if(epoch % evaluate_loss_after == 0):\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "            if(len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5\n",
    "                print(\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        model.U.data -= learning_rate * model.U.grad\n",
    "        model.V.data -= learning_rate * model.V.grad\n",
    "        model.W.data -= learning_rate * model.W.grad\n",
    "        num_examples_seen += len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533 ms ± 15 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "get_ipython().run_line_magic('timeit', 'model.calculate_loss(x_train[:5], y_train[:5]).backward()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-16 19:46:50: Loss after num_examples_seen=0 epoch=0: 8.986990\n",
      "2020-05-16 19:47:01: Loss after num_examples_seen=100 epoch=1: 8.986432\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = train_with_sgd(model, x_train[:100], y_train[:100], nepoch=2, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
