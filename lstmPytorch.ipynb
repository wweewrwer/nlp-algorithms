{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 79170 sentences.\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 4000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "max_len = 30\n",
    "\n",
    "print(\"Reading CSV file...\")\n",
    "with open('reddit-comments-2015-08.csv', 'rt', encoding= 'utf-8') as f:\n",
    "    reader = csv.reader(f, skipinitialspace = True)\n",
    "    next(reader)\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print(\"Parsed %d sentences.\" % (len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 64109 sentences with len 30.\n"
     ]
    }
   ],
   "source": [
    "max_len = 30\n",
    "ts = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "tokenized_sentences = []\n",
    "end_token = [sentence_end_token]*40\n",
    "for x in ts:\n",
    "    if len(x) > max_len:\n",
    "        continue\n",
    "    tokenized_sentences.append(x + end_token[:max_len-len(x)])\n",
    "print(\"Parsed %d sentences with len %d.\" % (len(tokenized_sentences),len(tokenized_sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45259 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Found %d unique words tokens.\" % len(list(word_freq.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 4000.\n",
      "The least frequent word in our vocabulary is 'pipe' and appeared 14 times.\n"
     ]
    }
   ],
   "source": [
    "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'UNKNOWN_TOKEN', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END', 'SENTENCE_END', 'SENTENCE_END', 'SENTENCE_END', 'SENTENCE_END', 'SENTENCE_END', 'SENTENCE_END', 'SENTENCE_END', 'SENTENCE_END', 'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "print(\"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print(\"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START i 'd like to make it available , but there 's no point in making it mandatory . SENTENCE_END SENTENCE_END SENTENCE_END SENTENCE_END SENTENCE_END SENTENCE_END SENTENCE_END SENTENCE_END SENTENCE_END SENTENCE_END\n",
      "[   1    5  143   40    6  100   10  723    4   24   47   16   65  173\n",
      "   14  289   10 2652    2    0    0    0    0    0    0    0    0    0\n",
      "    0]\n",
      "\n",
      "y:\n",
      "i 'd like to make it available , but there 's no point in making it mandatory . SENTENCE_END SENTENCE_END SENTENCE_END SENTENCE_END SENTENCE_END SENTENCE_END SENTENCE_END SENTENCE_END SENTENCE_END SENTENCE_END SENTENCE_END\n",
      "[   5  143   40    6  100   10  723    4   24   47   16   65  173   14\n",
      "  289   10 2652    2    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "x_example, y_example = x_train[17], y_train[17]\n",
    "print(\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
    "print(\"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "xx = torch.tensor(x_train,device=dev).long()\n",
    "yy = torch.tensor(y_train,device=dev).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUpytorch(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim=100, bptt_truncate=4):\n",
    "        super().__init__()\n",
    "        self.word_dim = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        self.gru = nn.GRU(vocab_size, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim,vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_train = F.one_hot(x, num_classes=self.word_dim)\n",
    "        ht,_ = self.gru(x_train.view(len(x_train),1,-1).float())\n",
    "        st = self.linear(ht.view(len(x_train),-1))\n",
    "        yt = F.log_softmax(st, dim=1)\n",
    "        return yt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_sgd(model, x_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    num_examples_seen = 0\n",
    "    calculate_loss=nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.005)\n",
    "    N = np.sum((len(y_i) for y_i in y_train))\n",
    "    for epoch in range(nepoch):\n",
    "        loss_total=0\n",
    "        for i in range(len(y_train)):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_hat=model.forward(x_train[i])\n",
    "            loss=calculate_loss(y_hat,y_train[i])\n",
    "\n",
    "            loss.backward()\n",
    "            loss_total+=loss\n",
    "            optimizer.step()\n",
    "            \n",
    "            num_examples_seen += 1\n",
    "        if(epoch % evaluate_loss_after == 0):\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss_total/N))\n",
    "            sys.stdout.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 10:21:40: Loss after num_examples_seen=1000 epoch=0: 0.185995\n",
      "2020-05-24 10:21:43: Loss after num_examples_seen=2000 epoch=1: 0.139535\n",
      "2020-05-24 10:21:47: Loss after num_examples_seen=3000 epoch=2: 0.131527\n",
      "2020-05-24 10:21:51: Loss after num_examples_seen=4000 epoch=3: 0.125992\n",
      "2020-05-24 10:21:54: Loss after num_examples_seen=5000 epoch=4: 0.121228\n",
      "2020-05-24 10:21:58: Loss after num_examples_seen=6000 epoch=5: 0.117645\n",
      "2020-05-24 10:22:02: Loss after num_examples_seen=7000 epoch=6: 0.115003\n",
      "2020-05-24 10:22:05: Loss after num_examples_seen=8000 epoch=7: 0.112906\n",
      "2020-05-24 10:22:09: Loss after num_examples_seen=9000 epoch=8: 0.111184\n",
      "2020-05-24 10:22:13: Loss after num_examples_seen=10000 epoch=9: 0.109741\n"
     ]
    }
   ],
   "source": [
    "model=GRUpytorch(vocabulary_size)\n",
    "model.to(dev)\n",
    "losses = train_with_sgd(model, xx[:1000], yy[:1000], nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 10:07:05: Loss after num_examples_seen=1000 epoch=0: 0.108356\n"
     ]
    }
   ],
   "source": [
    "model.to(dev)\n",
    "losses = train_with_sgd(model, xx[:1000], yy[:1000], nepoch=1, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "  with torch.no_grad():\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs = np.exp(model.forward(torch.tensor(new_sentence).long()))\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none `` sometimes among and society suppose\n",
      "harassment near post crimes interviews under and to . ? a for\n",
      "pace today awesome for president is .\n",
      "bunch os my for you on in . blocks\n",
      "damage filled vs gt , a , the it randomly lighter and see cell to , but suggesting haki you requests ,\n",
      "help flair blocks to transition rock models .\n",
      "wait google san . shadow to moment if in excited ,\n",
      "grass question a concrete insanely .\n",
      "hole be sad elder was provide place should instance the i based martin empire if assumed .\n",
      "convince u.s. /r/writingprompts i similarly\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 10\n",
    "senten_min_length = 5\n",
    "model.to('cpu')\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print(\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
