{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用原生pytorch实现GAT模型，论文为2018年的[《GRAPH ATTENTION NETWORKS》](https://arxiv.org/abs/1710.10903),该篇论文对算法的细节讲解非常详细，基本照着就可以把代码写出了来了，真是难得啊！看论文都要看哭了的我！  \n",
    "然后代码实现基本全程照抄以前的代码，太开心啦，框架照抄GCN的框架，attention详见transformer decoder部分的masked self-attention的部分，只是这次mask不是只mask时间序偏后的，而是mask不是邻接节点的就行了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#device = 'cpu'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用的数据集为[cora数据集](https://linqs.soe.ucsc.edu/data)(该网站还有其它关于图神经网络的数据集),该数据集由许多机器学习领域的paper构成，这些paper被分为7个类别，在该数据集中，一篇论文至少与该数据集中任一其它论文有引用或被引用关系，共2708篇论文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总共包含两个文件：  \n",
    "1.`cora.content`文件包含对paper的内容描述，格式为$$ \\text{<paper_id> <word_attributes> <class_label>} $$其中：  \n",
    "&emsp;`<paper_id>`是paper的标识符，每一篇paper对应一个标识符。  \n",
    "&emsp;`<word_attributes>`是词汇特征，为0或1，表示对应词汇是否存在。  \n",
    "&emsp;`<class_label>`是该文档所述的类别。  \n",
    "  \n",
    "2.`cora.cites`包含了数据集的引用图，格式为$$ \\text{<ID of cited paper> <ID of citing paper>} $$其中：  \n",
    "&emsp;`<ID of cited paper>`是被引用的paper标识符。  \n",
    "&emsp;`<ID of citing paper>`是引用的paper标识符。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "node_num, feat_dim, stat_dim, num_class, T\n",
    "feat_Matrix, X_Node, X_Neis, dg_list\n",
    "'''\n",
    "#数据处理\n",
    "content_path = \"./cora/cora.content\"\n",
    "cite_path = \"./cora/cora.cites\"\n",
    "\n",
    "#读取文本内容\n",
    "with open(content_path, \"r\") as fp:\n",
    "    contents = fp.readlines()\n",
    "with open(cite_path, \"r\") as fp:\n",
    "    cites = fp.readlines()\n",
    "    \n",
    "contents = np.array([np.array(s.strip().split(\"\\t\")) for s in contents])\n",
    "paper_list, feat_list, label_list = np.split(contents, [1,-1], axis=1)\n",
    "paper_list, label_list = np.squeeze(paper_list), np.squeeze(label_list)\n",
    "\n",
    "# paper -> index dict\n",
    "#print(\"paper_list\",sorted(paper_list))\n",
    "paper_dict = dict([(key, val) for val, key in enumerate(paper_list)])\n",
    "\n",
    "# lable -> index dict\n",
    "labels = list(set(label_list))\n",
    "label_dict = dict([(key, val) for val, key in enumerate(labels)])\n",
    "\n",
    "# edge_index\n",
    "cites = [i.strip().split(\"\\t\") for i in cites]\n",
    "#print(cites)\n",
    "#下面这几句代码不一样\n",
    "#print(paper_dict)\n",
    "cites = np.array([[paper_dict[i[0]], paper_dict[i[1]]] for i in cites], dtype = np.int64)\n",
    "#print(cites[1:7])\n",
    "cites = np.concatenate((cites, cites[:, ::-1]), axis=0) \n",
    "#print(cites[1:7])\n",
    "#这句也不一样\n",
    "#print(cites[:,0])\n",
    "degree_list=np.zeros(len(paper_list), dtype = np.int32)\n",
    "for i in cites:\n",
    "    degree_list[i[0]] += 1\n",
    "#_, degree_list = np.unique(cites[:,0],return_counts=True)\n",
    "#print(degree_list)\n",
    "\n",
    "#input\n",
    "node_num = len(paper_list)\n",
    "feat_dim = feat_list.shape[1]\n",
    "new_feat = 512\n",
    "num_class = len(labels)\n",
    "T = 2\n",
    "feat_Matrix = torch.Tensor(feat_list.astype(np.float32))\n",
    "X_Node, X_Neis = np.split(cites, 2, axis=1)\n",
    "X_Node, X_Neis = torch.tensor(np.squeeze(X_Node)), \\\n",
    "                 torch.tensor(np.squeeze(X_Neis))\n",
    "#print(X_Node)\n",
    "#print(degree_list[163])\n",
    "dg_list = torch.tensor(degree_list[X_Node])\n",
    "label_list = np.array([label_dict[i] for i in label_list])\n",
    "label_list = torch.tensor(label_list, dtype = torch.long, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of node :  2708\n",
      "Number of edges :  10858\n",
      "Number of classes :  7\n",
      "Dimension of node features :  1433\n",
      "Dimension of node state :  512\n",
      "Shape of feat_Matrix :  torch.Size([2708, 1433])\n",
      "Shape of X_Node :  torch.Size([10858])\n",
      "Shape of X_Neis :  torch.Size([10858])\n",
      "Length of dg_list :  10858\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of node : \", node_num)\n",
    "print(\"Number of edges : \", cites.shape[0])\n",
    "print(\"Number of classes : \", num_class)\n",
    "print(\"Dimension of node features : \", feat_dim)\n",
    "print(\"Dimension of node state : \", new_feat)\n",
    "print(\"Shape of feat_Matrix : \", feat_Matrix.shape)\n",
    "print(\"Shape of X_Node : \", X_Node.shape)\n",
    "print(\"Shape of X_Neis : \", X_Neis.shape)\n",
    "print(\"Length of dg_list : \", len(dg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Init:\n",
    "input_size:特征数\n",
    "new_size:输出的特征数\n",
    "head:head数\n",
    "Input:\n",
    "inputq,k,v: N * input_size\n",
    "mask: N * N, mask(i,j)表示j到i没有边\n",
    "Output: \n",
    "N * new_size\n",
    "\n",
    "注：1.论文中最后一层multihead过后不再是拼在一起，而是加在一起取平均，这里没有这么做，要实现也很简单的\n",
    "    2.论文中的attention与transformer的attention也不同，但是我觉得差别是不会太大的，仔细想一想这两种attention背后的意义，但是这些本来就比较玄学，\n",
    "    毕竟神经网络的可解释性很差，故不写上来了,下面代码图方便就直接用transformer的了\n",
    "'''\n",
    "class MultiHeadAttention(nn.Module):#目前是一个head的attention\n",
    "    def __init__(self, input_size, new_size, head = 2):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.hidden_size = new_size//head\n",
    "        assert head * self.hidden_size == new_size\n",
    "        self.new_size = new_size\n",
    "        self.head = head\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.qlinear = nn.Linear(input_size,new_size)\n",
    "        self.vlinear = nn.Linear(input_size,new_size)\n",
    "        self.klinear = nn.Linear(input_size,new_size)\n",
    "        self.ylinear = nn.Linear(new_size,new_size)\n",
    "        \n",
    "    def scaledDotProductAttention(self, matq, matk, matv, mask):\n",
    "        scale = matk.size(-1)**0.5\n",
    "        matt = matq.matmul(matk.transpose(1,2))/scale  #(head, N,hidden_size) * (head, hidden_size,N) -> (head, N,N)\n",
    "        if not mask is None:\n",
    "            matt.masked_fill_(mask, -np.inf)\n",
    "        matt = F.softmax(matt, dim=-1)  #(head, N, N) * (head, N, hidde_size)\n",
    "        mattv = matt.matmul(matv)\n",
    "        return torch.cat(torch.chunk(mattv.view(-1,self.hidden_size),self.head,0), dim = 1) #(head, N, hidden_size) ->(N, new_size)\n",
    "    \n",
    "    def toMulti(self, matx):#这里是把特征等分\n",
    "        return torch.cat(torch.chunk(matx, self.head, -1),dim=0).view(self.head, -1, self.hidden_size)\n",
    "    \n",
    "    def forward(self, inputq, inputk, inputv, mask = None):\n",
    "        matq = self.toMulti(self.qlinear(inputq))\n",
    "        matv = self.toMulti(self.vlinear(inputv))\n",
    "        matk = self.toMulti(self.klinear(inputk))\n",
    "        mattv = self.scaledDotProductAttention(matq, matk, matv, mask)\n",
    "        y = self.ylinear(mattv)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5229, 0.5034],\n",
      "        [0.4926, 0.4687],\n",
      "        [0.3847, 0.3493],\n",
      "        [0.3212, 0.3451],\n",
      "        [0.3911, 0.3379],\n",
      "        [0.3800, 0.2676]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "#开始检查没有mask的对不对\n",
    "model1 = MultiHeadAttention(5, 2).to(device)\n",
    "input1 = torch.randn(6,5).to(device)\n",
    "y1 = model1(input1,input1,input1)\n",
    "print(y1)\n",
    "torch.sum(y1).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5505, 0.6872],\n",
      "        [0.5592, 0.6937],\n",
      "        [0.5571, 0.5888],\n",
      "        [0.5451, 0.5281],\n",
      "        [0.5635, 0.5805],\n",
      "        [0.5562, 0.6307]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "#开始检查有mask的对不对\n",
    "model1 = MultiHeadAttention(5, 2).to(device)\n",
    "input1 = torch.randn(6,5).to(device)\n",
    "y1 = model1(input1,input1,input1,torch.tensor([[False,False,False,False,False,True]]*6, device=device))\n",
    "print(y1)\n",
    "torch.sum(y1).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Init:\n",
    "    feat_dim : 特征数\n",
    "    new_feat : 输出的新节点的特征数\n",
    "Input:\n",
    "    H : (N, fear_dim), N个节点的特征\n",
    "    A : (N, N), 邻接矩阵，用以在对应的地方产生mask\n",
    "    \n",
    "'''\n",
    "class GATlayer(nn.Module):\n",
    "    def __init__(self, feat_dim, new_feat):\n",
    "        super(GATlayer, self).__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        self.new_feat = new_feat\n",
    "        \n",
    "        self.attention = MultiHeadAttention(feat_dim, new_feat)\n",
    "    \n",
    "    def forward(self, H, A):\n",
    "        return F.relu(self.attention(H,H,H,A.eq(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, feat_dim, new_feat, num_class):\n",
    "        super(GAT,self).__init__()\n",
    "        \n",
    "        self.gat_layer1 = GATlayer(feat_dim, new_feat)\n",
    "        self.gat_layer2 = GATlayer(new_feat, new_feat)\n",
    "        self.out_layer = nn.Linear(new_feat, num_class)\n",
    "        #这里天坑，由于不需要学习的我一向喜欢直接用F.而不是类\n",
    "        #结果之前一直准确率不对，才发现如果用的是类在预测时会自动关闭的，而函数则不会\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "    \n",
    "    def forward(self, X, A):\n",
    "        H = self.gat_layer1(X, A)\n",
    "        H = self.gat_layer2(H, A)\n",
    "        output = F.log_softmax(self.dropout(self.out_layer(H)),dim = -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edgeToMat(edges, node_num):\n",
    "    A = torch.eye(node_num).to(device)\n",
    "    for i in edges:\n",
    "        A[i[0]][i[1]] += 1\n",
    "        A[i[1]][i[0]] += 1\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [0., 1., 1., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 0., 1., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a1= [[0,2],[0,3],[1,2],[2,3]]\n",
    "print(edgeToMat(a1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,feat_Matrix,A,train_mask,test_mask,learning_rate = 0.01, weight_decay = 1e-3):\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(feat_Matrix, A)\n",
    "\n",
    "        loss = F.nll_loss(out[train_mask], label_list[train_mask])\n",
    "        _, pred = out.max(dim=1)\n",
    "\n",
    "        correct = float(pred[train_mask].eq(label_list[train_mask]).sum().item())\n",
    "        acc = correct / train_mask.sum().item()\n",
    "        if epoch % 10 == 0:\n",
    "            print('[Epoch {}/100] Loss {:.4f}, train acc {:.4f}'.format(epoch, loss.cpu().detach().data.item(), acc))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            model.eval()\n",
    "            _, pred = model(feat_Matrix, A).max(dim = 1)\n",
    "            correct = float(pred[test_mask].eq(label_list[test_mask]).sum().item())\n",
    "            acc = correct / test_mask.sum().item()\n",
    "            print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] Loss 1.9400, train acc 0.2324\n",
      "[Epoch 10/200] Loss 1.6778, train acc 0.2670\n",
      "Accuracy: 0.2980\n",
      "[Epoch 20/200] Loss 1.5534, train acc 0.3402\n",
      "[Epoch 30/200] Loss 1.2507, train acc 0.4578\n",
      "Accuracy: 0.6120\n",
      "[Epoch 40/200] Loss 1.0585, train acc 0.5345\n",
      "[Epoch 50/200] Loss 0.6797, train acc 0.7207\n",
      "Accuracy: 0.7560\n",
      "[Epoch 60/200] Loss 0.4866, train acc 0.7687\n",
      "[Epoch 70/200] Loss 0.4091, train acc 0.7963\n",
      "Accuracy: 0.7540\n",
      "[Epoch 80/200] Loss 0.3747, train acc 0.8044\n",
      "[Epoch 90/200] Loss 3.2781, train acc 0.5287\n",
      "Accuracy: 0.6420\n",
      "[Epoch 100/200] Loss 1.1929, train acc 0.6142\n",
      "[Epoch 110/200] Loss 0.5371, train acc 0.7681\n",
      "Accuracy: 0.7880\n",
      "[Epoch 120/200] Loss 0.4266, train acc 0.7851\n",
      "[Epoch 130/200] Loss 0.3717, train acc 0.8056\n",
      "Accuracy: 0.7680\n",
      "[Epoch 140/200] Loss 0.3470, train acc 0.8179\n",
      "[Epoch 150/200] Loss 0.3202, train acc 0.8220\n",
      "Accuracy: 0.7560\n",
      "[Epoch 160/200] Loss 0.3383, train acc 0.8132\n",
      "[Epoch 170/200] Loss 0.3079, train acc 0.8314\n",
      "Accuracy: 0.7460\n",
      "[Epoch 180/200] Loss 0.2944, train acc 0.8308\n",
      "[Epoch 190/200] Loss 12.5138, train acc 0.2248\n",
      "Accuracy: 0.4220\n"
     ]
    }
   ],
   "source": [
    "#split dataset\n",
    "train_mask = torch.zeros(node_num, dtype = torch.bool)\n",
    "train_mask[:node_num - 1000] = 1               #1700左右training\n",
    "\n",
    "val_mask = None                                \n",
    "test_mask = torch.zeros(node_num, dtype = torch.bool)\n",
    "test_mask[node_num - 500:] = 1                 # 500test\n",
    "\n",
    "model = GAT(feat_dim, new_feat, num_class).to(device)\n",
    "\n",
    "#Adam是一种算法，可以百度了解\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01, weight_decay = 1e-3)\n",
    "feat_Matrix = feat_Matrix.to(device)\n",
    "A = edgeToMat(cites, node_num).to(device)\n",
    "train(model,optimizer,feat_Matrix,A,train_mask,test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] Loss 1.5971, train acc 0.3454\n",
      "[Epoch 10/100] Loss 1.3206, train acc 0.4321\n",
      "Accuracy: 0.5080\n",
      "[Epoch 20/100] Loss 1.2189, train acc 0.4801\n",
      "[Epoch 30/100] Loss 1.1940, train acc 0.4830\n",
      "Accuracy: 0.4680\n",
      "[Epoch 40/100] Loss 1.1949, train acc 0.4748\n",
      "[Epoch 50/100] Loss 1.1979, train acc 0.4813\n",
      "Accuracy: 0.4840\n",
      "[Epoch 60/100] Loss 1.1786, train acc 0.4994\n",
      "[Epoch 70/100] Loss 1.1455, train acc 0.5117\n",
      "Accuracy: 0.4720\n",
      "[Epoch 80/100] Loss 1.2571, train acc 0.4719\n",
      "[Epoch 90/100] Loss 1.2137, train acc 0.4684\n",
      "Accuracy: 0.5300\n"
     ]
    }
   ],
   "source": [
    "train(model,optimizer,feat_Matrix,A,train_mask,test_mask,learning_rate = 0.01, weight_decay = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
