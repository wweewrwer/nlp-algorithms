{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用原生pytorch实现最原生的linear GNN模型，论文为2009年的[《The graph neural network model》](https://persagen.com/files/misc/scarselli2009graph.pdf)的GNN,\n",
    "代码部分大幅借鉴了[Github用户LYuhang](https://github.com/LYuhang/GNN_Review/blob/master/PyG%E5%92%8CPytorch%E5%AE%9E%E7%8E%B0GNN%E6%A8%A1%E5%9E%8B/GNN_Implement_with_Pytorch.ipynb),建议大家以他的讲解为主，我的作为一些细节的补充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "#Tqdm 是一个快速，可扩展的Python进度条，可以在 Python 长循环中添加一个进度提示信息，用户只需要封装任意的迭代器 tqdm(iterator)\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cpu'\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用的数据集为[cora数据集](https://linqs.soe.ucsc.edu/data)(该网站还有其它关于图神经网络的数据集),该数据集由许多机器学习领域的paper构成，这些paper被分为7个类别，在该数据集中，一篇论文至少与该数据集中任一其它论文有引用或被引用关系，共2708篇论文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总共包含两个文件：  \n",
    "1.`cora.content`文件包含对paper的内容描述，格式为$$ \\text{<paper_id> <word_attributes> <class_label>} $$其中：  \n",
    "&emsp;`<paper_id>`是paper的标识符，每一篇paper对应一个标识符。  \n",
    "&emsp;`<word_attributes>`是词汇特征，为0或1，表示对应词汇是否存在。  \n",
    "&emsp;`<class_label>`是该文档所述的类别。  \n",
    "  \n",
    "2.`cora.cites`包含了数据集的引用图，格式为$$ \\text{<ID of cited paper> <ID of citing paper>} $$其中：  \n",
    "&emsp;`<ID of cited paper>`是被引用的paper标识符。  \n",
    "&emsp;`<ID of citing paper>`是引用的paper标识符。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "node_num, feat_dim, stat_dim, num_class, T\n",
    "feat_Matrix, X_Node, X_Neis, dg_list, label_egde\n",
    "'''\n",
    "#数据处理\n",
    "content_path = \"./cora/cora.content\"\n",
    "cite_path = \"./cora/cora.cites\"\n",
    "\n",
    "#读取文本内容\n",
    "with open(content_path, \"r\") as fp:\n",
    "    contents = fp.readlines()\n",
    "with open(cite_path, \"r\") as fp:\n",
    "    cites = fp.readlines()\n",
    "    \n",
    "contents = np.array([np.array(s.strip().split(\"\\t\")) for s in contents])\n",
    "paper_list, feat_list, label_list = np.split(contents, [1,-1], axis=1)\n",
    "paper_list, label_list = np.squeeze(paper_list), np.squeeze(label_list)\n",
    "\n",
    "# paper -> index dict\n",
    "#print(\"paper_list\",sorted(paper_list))\n",
    "paper_dict = dict([(key, val) for val, key in enumerate(paper_list)])\n",
    "\n",
    "# lable -> index dict\n",
    "labels = list(set(label_list))\n",
    "label_dict = dict([(key, val) for val, key in enumerate(labels)])\n",
    "\n",
    "# edge_index\n",
    "cites = [i.strip().split(\"\\t\") for i in cites]\n",
    "#print(cites)\n",
    "#下面这几句代码不一样\n",
    "#print(paper_dict)\n",
    "cites = np.array([[paper_dict[i[0]], paper_dict[i[1]]] for i in cites], dtype = np.int64)\n",
    "#print(cites[1:7])\n",
    "cites = np.concatenate((cites, cites[:, ::-1]), axis=0) \n",
    "#print(cites[1:7])\n",
    "#这句也不一样\n",
    "#print(cites[:,0])\n",
    "degree_list=np.zeros(len(paper_list), dtype = np.int32)\n",
    "for i in cites:\n",
    "    degree_list[i[0]] += 1\n",
    "#_, degree_list = np.unique(cites[:,0],return_counts=True)\n",
    "#print(degree_list)\n",
    "\n",
    "#input\n",
    "node_num = len(paper_list)\n",
    "feat_dim = feat_list.shape[1]\n",
    "stat_dim = 32\n",
    "num_class = len(labels)\n",
    "edge_dim = 2\n",
    "T = 2\n",
    "feat_Matrix = torch.Tensor(feat_list.astype(np.float32))\n",
    "X_Node, X_Neis = np.split(cites, 2, axis=1)\n",
    "X_Node, X_Neis = torch.tensor(np.squeeze(X_Node)), \\\n",
    "                 torch.tensor(np.squeeze(X_Neis))\n",
    "#print(X_Node)\n",
    "#print(degree_list[163])\n",
    "dg_list = torch.tensor(degree_list[X_Node])\n",
    "label_list = np.array([label_dict[i] for i in label_list])\n",
    "label_list = torch.tensor(label_list, dtype = torch.long)\n",
    "label_edge = torch.tensor([[0., 1.]] * (len(dg_list)//2)+[[1., 0.]] * (len(dg_list)//2))  #这个ont-hot向量分别表示引用边和被引用边\n",
    "#print(label_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of node :  2708\n",
      "Number of edges :  10858\n",
      "Number of classes :  7\n",
      "Dimension of node features :  1433\n",
      "Dimension of node state :  32\n",
      "Shape of feat_Matrix :  torch.Size([2708, 1433])\n",
      "Shape of X_Node :  torch.Size([10858])\n",
      "Shape of X_Neis :  torch.Size([10858])\n",
      "Length of dg_list :  10858\n",
      "Length of label_edge :  10858\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of node : \", node_num)\n",
    "print(\"Number of edges : \", cites.shape[0])\n",
    "print(\"Number of classes : \", num_class)\n",
    "print(\"Dimension of node features : \", feat_dim)\n",
    "print(\"Dimension of node state : \", stat_dim)\n",
    "print(\"Shape of feat_Matrix : \", feat_Matrix.shape)\n",
    "print(\"Shape of X_Node : \", X_Node.shape)\n",
    "print(\"Shape of X_Neis : \", X_Neis.shape)\n",
    "print(\"Length of dg_list : \", len(dg_list))\n",
    "print(\"Length of label_edge : \", len(label_edge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](equation1.jpg)\n",
    "![avatar](equation2.jpg)\n",
    "![avatar](equation3.jpg)\n",
    "其中，关于各参数的解释见论文第10页,以下附上截图：  \n",
    "其中s是state的维度\n",
    "![avatar](linearGNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "下面Xi函数是生成论文中A矩阵方程式中的Xi矩阵(就是符号很奇怪的那个矩阵）。\n",
    "然后我的phi函数是直接把ln,lu拼接起来，\n",
    "l(n,u)没有（其实这里应该加上更好，引用边和被引用边显然应该用不同的label）\n",
    "Initialization :\n",
    "Input :\n",
    "    ln : (int)特征向量维度\n",
    "    le : (int)边的特征向量维度\n",
    "    s : (int)状态向量维度\n",
    "Forward : \n",
    "N为节点数\n",
    "Input :\n",
    "    input : (Tensor)节点对(i,j)的特征向量拼接起来，shape为(N，2*ln+le)\n",
    "Output :\n",
    "    out : (Tensor)Xi矩阵，shape为(N, s, s)\n",
    "'''\n",
    "class Xi(nn.Module):\n",
    "    def __init__(self, ln, s, le):\n",
    "        super(Xi, self).__init__()\n",
    "        self.ln = ln\n",
    "        self.s = s\n",
    "        \n",
    "        self.linear = nn.Linear(2 * ln + le, s ** 2)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        #在jupyter中用F.tanh会有警告，它会让你用torch.tanh\n",
    "        #它的警告是F.tanh is deprecated,但是我网上都没有查到这个\n",
    "        #不过反正torch.tanh是一样的，那就不用F.tanh了\n",
    "        output = torch.tanh(self.linear(input))\n",
    "        return output.view(-1, self.s, self.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6787, -0.1234,  0.0259],\n",
      "         [-0.4561, -0.9040, -0.1219],\n",
      "         [ 0.2899,  0.2537, -0.4306]],\n",
      "\n",
      "        [[-0.4246,  0.2783,  0.0912],\n",
      "         [ 0.7354, -0.2594, -0.4767],\n",
      "         [-0.5070,  0.0617, -0.4514]],\n",
      "\n",
      "        [[ 0.6619, -0.2249,  0.4534],\n",
      "         [-0.8988,  0.3727, -0.4289],\n",
      "         [ 0.4077, -0.1982, -0.8739]],\n",
      "\n",
      "        [[ 0.2755,  0.4321,  0.0447],\n",
      "         [ 0.3188, -0.3134, -0.7724],\n",
      "         [ 0.0145, -0.4467, -0.6117]]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "#测试正确与否：\n",
    "model1 = Xi(5,3 ,2)\n",
    "input1 = torch.randn(4,12)\n",
    "print(model1(input1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "实现论文中rou矩阵，并生成偏置项b\n",
    "Initialization :\n",
    "Input :\n",
    "    ln : (int)特征向量维度\n",
    "    s : (int)状态向量维度\n",
    "Forward :\n",
    "Input :\n",
    "    input : (Tensor)节点的特征向量矩阵，shape(N, ln)\n",
    "Output :\n",
    "    out : (Tensor)偏置矩阵，shape(N, s)\n",
    "'''\n",
    "class Rou(nn.Module):\n",
    "    def __init__(self, ln, s):\n",
    "        super(Rou, self).__init__()\n",
    "        self.linear = nn.Linear(ln,s)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return torch.tanh(self.linear(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3827,  0.2693, -0.4215],\n",
      "        [ 0.4116,  0.5820,  0.0045],\n",
      "        [-0.3160,  0.1672,  0.3444],\n",
      "        [ 0.2149,  0.5229, -0.0913]], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "model1 = Rou(5,3)\n",
    "input1 = torch.randn(4,5)\n",
    "print(model1(input1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "实现Hw函数，就是论文中的公式12,其中|ne[u]|的意思是ne[u]邻居的个数\n",
    "Initialize :\n",
    "Input :\n",
    "    ln : (int)节点特征向量维度\n",
    "    s : (int)节点状态向量维度\n",
    "    le : (int)边的特征向量维度\n",
    "    mu : (int)设定的压缩映射的压缩系数\n",
    "Forward :\n",
    "Input :\n",
    "    X : (Tensor)每一行为一条边的两个节点特征向量外加这条边的特征向量连接起来得到的向量，shape为(N, 2*ln + le)\n",
    "    H : (Tensor)与X每行对应u=one of ne[n](即x中的第二个节点,这里默认边是第二个点连到第一个，即2是1的ne,但1不是2的ne)的状态向量\n",
    "    dg_list: (Tensor)与X每行对应u=one of ne[n]的度数向量ne[u]\n",
    "Output :\n",
    "    out : (Tensor)Hw函数的输出\n",
    "'''\n",
    "class Hw(nn.Module):\n",
    "    def __init__(self, ln, s, le, mu=0.9):\n",
    "        super(Hw, self).__init__()\n",
    "        self.ln = ln\n",
    "        self.s = s\n",
    "        self.le = le\n",
    "        self.mu = mu\n",
    "        \n",
    "        self.Xi = Xi(ln, s, le)\n",
    "        self.Rou = Rou(ln, s)\n",
    "    \n",
    "    '''\n",
    "    A: N * s * s\n",
    "    b: N * s\n",
    "    '''\n",
    "    def forward(self, X, H, dg_list):\n",
    "        A = (self.Xi(X) * self.mu / self.s)/dg_list.view(-1, 1, 1)\n",
    "        b = self.Rou(X[:,0:self.ln])#chunk函数：分割tensor，(n,ln)->(n,s)\n",
    "        #下面必须这样先unsqueeze再squeeze,可以参考matmul规则\n",
    "        #matmul高维矩阵相乘：自动在前补1*，使维数相同，然后广播，使高于最后两维的size相同\n",
    "        #然后依次对里面进行二维的矩阵乘法\n",
    "        #如(j*1*n*p)(k*p*m)->(j*k*n*p)(j*k*p*m)->(j*k*n*m) \n",
    "        output = torch.squeeze(torch.matmul(A, torch.unsqueeze(H,2)),-1) + b\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0025, -0.1701, -0.1836],\n",
      "        [ 0.6183,  0.1260, -0.0162],\n",
      "        [ 0.0338, -0.6600,  0.1344],\n",
      "        [ 0.4552, -0.1040, -0.1249]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model1 = Hw(5,3,2)\n",
    "X1 = torch.randn(4,12)\n",
    "H1 = torch.randn(4,3)\n",
    "dg_list1 = torch.tensor([2,3,4,5])\n",
    "print(model1(X1, H1, dg_list1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "实现上面论文中的方程（3），将前面使用的hw函数得到的信息相加了，\n",
    "并更新每一个节点的状态向量\n",
    "Initialize :\n",
    "Input :\n",
    "    node_num : (int)节点的数量\n",
    "Forward :\n",
    "Input :\n",
    "    H : (Tensor)Hw的输出，shape为(E, s)\n",
    "    X_node : (Tensor)H每一行对应要更新的节点的索引(E)\n",
    "Output :\n",
    "    out :(Tensor)新的节点状态向量，shape为(N, s), N为节点个数\n",
    "'''\n",
    "class AggrSum(nn.Module):\n",
    "    def __init__(self, node_num):\n",
    "        super(AggrSum, self).__init__()\n",
    "        self.N = node_num\n",
    "        \n",
    "    def forward(self, H, X_node):\n",
    "        #H : (E, s) -> (N,s)\n",
    "        #感觉写出矩阵乘法有一点点浪费时间和空间\n",
    "        #但是for循环寻址也挺慢的,方便来看还是写矩阵乘法吧\n",
    "        mask = torch.stack([X_node] * self.N, 0)\n",
    "        mask = mask.float() - torch.unsqueeze(torch.arange(0,self.N).float(), 1)\n",
    "        mask = (mask == 0).float()\n",
    "        # (V, N) * (N, s) -> (V, s)\n",
    "        return torch.mm(mask, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1437,  0.5951,  0.3575],\n",
      "        [-0.8037,  0.6757,  0.4867],\n",
      "        [-0.3223,  0.2060,  0.3875],\n",
      "        [ 0.2539, -0.9173,  0.1773]], grad_fn=<AddBackward0>)\n",
      "tensor([[ 0.1101, -0.3221,  0.5348],\n",
      "        [-1.1260,  0.8818,  0.8742]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "model1 = Hw(5,3,2)\n",
    "X1 = torch.randn(4,12)\n",
    "H1 = torch.randn(4,3)\n",
    "dg_list1 = torch.tensor([2,3,4,5])\n",
    "print(model1(X1, H1, dg_list1))\n",
    "model2 = AggrSum(2)\n",
    "print(model2(model1(X1, H1, dg_list1), torch.tensor([0,1,1,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "实现Linear GNN模型，循环迭代计算T次，\n",
    "达到不动点之后，使用线性函数得到输出，进行分类\n",
    "Initialize : \n",
    "Input :\n",
    "    node_num : (int)节点个数\n",
    "    feat_dim : (int)节点特征向量维度\n",
    "    stat_dim : (int)节点状态向量维度\n",
    "    edge_dim : (int)边特征向量维度\n",
    "    T : (int)迭代计算的次数\n",
    "Forward :\n",
    "Input :\n",
    "    feat_Matrix : (Tensor)节点的特征矩阵，shape为（N，ln)\n",
    "    X_Node : (Tensor)每条边的提供更新的节点的索引，例如i->j,i就是提供更新的，（N）\n",
    "    X_Neis : (Tensor)每条边被更新的节点的索引，（N）\n",
    "    dg_list : (Tensor)与X_Node对应节点的度列表，shape为（N）\n",
    "    label_edge : (Tensor）特征向量\n",
    "    out : (Tensor)每个节点的类别概率，shape为（V，num_class)\n",
    "'''\n",
    "class OriLinearGNN(nn.Module):\n",
    "    def __init__(self, node_num, feat_dim, stat_dim, num_class, edge_dim,T):\n",
    "        super(OriLinearGNN, self).__init__()\n",
    "        self.embed_dim = feat_dim\n",
    "        self.stat_dim = stat_dim\n",
    "        self.edge_dim = edge_dim\n",
    "        self.T = T\n",
    "        \n",
    "        self.out_layer = nn.Linear(stat_dim, num_class)\n",
    "        #这里天坑，由于不需要学习的我一向喜欢直接用F.而不是类\n",
    "        #结果之前一直准确率不对，才发现如果用的是类在预测时会自动关闭的，而函数则不会\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        self.Hw = Hw(feat_dim, stat_dim, edge_dim)\n",
    "        self.Aggr = AggrSum(node_num)\n",
    "        \n",
    "    def forward(self, feat_Matrix, X_Node, X_Neis, dg_list, label_edge):\n",
    "        #这里是取出要用到的特征向量\n",
    "        node_embeds = torch.index_select(feat_Matrix, 0, X_Node)\n",
    "        neis_embeds = torch.index_select(feat_Matrix, 0, X_Neis)\n",
    "        X = torch.cat((node_embeds, neis_embeds, label_edge), 1) #N, 2*ln + le\n",
    "        #H是其中的状态向量\n",
    "        H = torch.zeros((feat_Matrix.shape[0], self.stat_dim), dtype = torch.float32).to(device)\n",
    "        for t in range(self.T):\n",
    "            #(V, s) -> (N, s)\n",
    "            #这里是取出要用到的状态向量\n",
    "            H = torch.index_select(H, 0, X_Neis)\n",
    "            #(N, s) -> (N, s)\n",
    "            #这里是得到能拿来更新状态向量的矩阵\n",
    "            H = self.Hw(X, H, dg_list)\n",
    "            #(N, s) -> (V, s)\n",
    "            #这里是更新状态向量\n",
    "            H = self.Aggr(H, X_Node)\n",
    "        output = F.log_softmax(self.dropout(self.out_layer(H)),dim = -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] Loss 1.9906, train acc 0.1399\n",
      "[Epoch 1/200] Loss 1.5634, train acc 0.4251\n",
      "[Epoch 2/200] Loss 1.3289, train acc 0.5281\n",
      "[Epoch 3/200] Loss 1.1532, train acc 0.5738\n",
      "[Epoch 4/200] Loss 1.0439, train acc 0.5937\n",
      "[Epoch 5/200] Loss 1.0078, train acc 0.5896\n",
      "[Epoch 6/200] Loss 0.9554, train acc 0.5937\n",
      "[Epoch 7/200] Loss 0.8937, train acc 0.5972\n",
      "[Epoch 8/200] Loss 0.8687, train acc 0.6107\n",
      "[Epoch 9/200] Loss 0.8346, train acc 0.6001\n",
      "Accuracy: 0.7720\n",
      "[Epoch 10/200] Loss 0.7979, train acc 0.6224\n",
      "[Epoch 11/200] Loss 0.8005, train acc 0.6189\n",
      "[Epoch 12/200] Loss 0.8298, train acc 0.5978\n",
      "[Epoch 13/200] Loss 0.7673, train acc 0.6276\n",
      "[Epoch 14/200] Loss 0.7700, train acc 0.6388\n",
      "[Epoch 15/200] Loss 0.7755, train acc 0.6171\n",
      "[Epoch 16/200] Loss 0.7395, train acc 0.6370\n",
      "[Epoch 17/200] Loss 0.7758, train acc 0.6183\n",
      "[Epoch 18/200] Loss 0.7412, train acc 0.6265\n",
      "[Epoch 19/200] Loss 0.7543, train acc 0.6253\n",
      "Accuracy: 0.7780\n",
      "[Epoch 20/200] Loss 0.7376, train acc 0.6335\n",
      "[Epoch 21/200] Loss 0.7483, train acc 0.6300\n",
      "[Epoch 22/200] Loss 0.7354, train acc 0.6118\n",
      "[Epoch 23/200] Loss 0.7438, train acc 0.6294\n",
      "[Epoch 24/200] Loss 0.7553, train acc 0.6241\n",
      "[Epoch 25/200] Loss 0.7166, train acc 0.6300\n",
      "[Epoch 26/200] Loss 0.6939, train acc 0.6563\n",
      "[Epoch 27/200] Loss 0.7549, train acc 0.6071\n",
      "[Epoch 28/200] Loss 0.6930, train acc 0.6423\n",
      "[Epoch 29/200] Loss 0.7155, train acc 0.6206\n",
      "Accuracy: 0.7720\n",
      "[Epoch 30/200] Loss 0.7193, train acc 0.6341\n",
      "[Epoch 31/200] Loss 0.7362, train acc 0.6206\n",
      "[Epoch 32/200] Loss 0.7084, train acc 0.6341\n",
      "[Epoch 33/200] Loss 0.6792, train acc 0.6399\n",
      "[Epoch 34/200] Loss 0.7130, train acc 0.6358\n",
      "[Epoch 35/200] Loss 0.7082, train acc 0.6194\n",
      "[Epoch 36/200] Loss 0.6883, train acc 0.6376\n",
      "[Epoch 37/200] Loss 0.7211, train acc 0.6270\n",
      "[Epoch 38/200] Loss 0.6896, train acc 0.6376\n",
      "[Epoch 39/200] Loss 0.6931, train acc 0.6388\n",
      "Accuracy: 0.7640\n",
      "[Epoch 40/200] Loss 0.6726, train acc 0.6528\n",
      "[Epoch 41/200] Loss 0.6835, train acc 0.6370\n",
      "[Epoch 42/200] Loss 0.6977, train acc 0.6306\n",
      "[Epoch 43/200] Loss 0.6818, train acc 0.6487\n",
      "[Epoch 44/200] Loss 0.7232, train acc 0.6159\n",
      "[Epoch 45/200] Loss 0.6808, train acc 0.6458\n",
      "[Epoch 46/200] Loss 0.6819, train acc 0.6352\n",
      "[Epoch 47/200] Loss 0.7056, train acc 0.6253\n",
      "[Epoch 48/200] Loss 0.7290, train acc 0.6189\n",
      "[Epoch 49/200] Loss 0.6876, train acc 0.6259\n",
      "Accuracy: 0.7480\n"
     ]
    }
   ],
   "source": [
    "#split dataset\n",
    "train_mask = torch.zeros(node_num, dtype = torch.bool)\n",
    "train_mask[:node_num - 1000] = 1               #1700左右training\n",
    "\n",
    "val_mask = None                                \n",
    "test_mask = torch.zeros(node_num, dtype = torch.bool)\n",
    "test_mask[node_num - 500:] = 1                 # 500test\n",
    "\n",
    "model = OriLinearGNN(node_num, feat_dim, stat_dim, num_class, edge_dim, T).to(device)\n",
    "\n",
    "#Adam是一种算法，可以百度了解\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01, weight_decay = 1e-3)\n",
    "feat_Matrix = feat_Matrix.to(device)\n",
    "X_Node = X_Node.to(device)\n",
    "X_Neis = X_Neis.to(device)\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(feat_Matrix, X_Node, X_Neis, dg_list, label_edge)\n",
    "    \n",
    "    loss = F.nll_loss(out[train_mask], label_list[train_mask])\n",
    "    _, pred = out.max(dim=1)\n",
    "    \n",
    "    correct = float(pred[train_mask].eq(label_list[train_mask]).sum().item())\n",
    "    acc = correct / train_mask.sum().item()\n",
    "    print('[Epoch {}/200] Loss {:.4f}, train acc {:.4f}'.format(epoch, loss.cpu().detach().data.item(), acc))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        _, pred = model(feat_Matrix, X_Node, X_Neis, dg_list, label_edge).max(dim = 1)\n",
    "        correct = float(pred[test_mask].eq(label_list[test_mask]).sum().item())\n",
    "        acc = correct / test_mask.sum().item()\n",
    "        print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
