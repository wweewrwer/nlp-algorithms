{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[169 169 169 ...   2   2   3]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "node_num, feat_dim, stat_dim, num_class, T\n",
    "feat_Matrix, X_Node, X_Neis, dg_list\n",
    "'''\n",
    "content_path = \"./cora/cora.content\"\n",
    "cite_path = \"./cora/cora.cites\"\n",
    "\n",
    "# 读取文本内容\n",
    "with open(content_path, \"r\") as fp:\n",
    "    contents = fp.readlines()\n",
    "with open(cite_path, \"r\") as fp:\n",
    "    cites = fp.readlines()\n",
    "\n",
    "contents = np.array([np.array(l.strip().split(\"\\t\")) for l in contents])\n",
    "paper_list, feat_list, label_list = np.split(contents, [1,-1], axis=1)\n",
    "paper_list, label_list = np.squeeze(paper_list), np.squeeze(label_list)\n",
    "# Paper -> Index dict\n",
    "paper_dict = dict([(key, val) for val, key in enumerate(paper_list)])\n",
    "# Label -> Index 字典\n",
    "labels = list(set(label_list))\n",
    "label_dict = dict([(key, val) for val, key in enumerate(labels)])\n",
    "# Edge_index\n",
    "cites = [i.strip().split(\"\\t\") for i in cites]\n",
    "cites = np.array([[paper_dict[i[0]], paper_dict[i[1]]] for i in cites], \n",
    "                 np.int64).T   # (2, edge)\n",
    "cites = np.concatenate((cites, cites[::-1, :]), axis=1)  # (2, 2*edge) or (2, E)\n",
    "# Degree\n",
    "_, degree_list = np.unique(cites[0,:], return_counts=True)\n",
    "\n",
    "# Input\n",
    "node_num = len(paper_list)\n",
    "feat_dim = feat_list.shape[1]\n",
    "stat_dim = 32\n",
    "num_class = len(labels)\n",
    "T = 2\n",
    "feat_Matrix = torch.Tensor(feat_list.astype(np.float32))\n",
    "X_Node, X_Neis = np.split(cites, 2, axis=0)\n",
    "X_Node, X_Neis = torch.from_numpy(np.squeeze(X_Node)), \\\n",
    "                 torch.from_numpy(np.squeeze(X_Neis))\n",
    "dg_list = degree_list[X_Node]\n",
    "label_list = np.array([label_dict[i] for i in label_list])\n",
    "label_list = torch.tensor(label_list, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Data Process Info********************\n",
      "==> Number of node : 2708\n",
      "==> Number of edges : 10858/2=5429\n",
      "==> Number of classes : 7\n",
      "==> Dimension of node features : 1433\n",
      "==> Dimension of node state : 32\n",
      "==> T : 2\n",
      "==> Shape of feat_Matrix : torch.Size([2708, 1433])\n",
      "==> Shape of X_Node : torch.Size([10858])\n",
      "==> Shape of X_Neis : torch.Size([10858])\n",
      "==> Length of dg_list : 10858\n"
     ]
    }
   ],
   "source": [
    "print(\"{}Data Process Info{}\".format(\"*\"*20, \"*\"*20))\n",
    "print(\"==> Number of node : {}\".format(node_num))\n",
    "print(\"==> Number of edges : {}/2={}\".format(cites.shape[1], int(cites.shape[1]/2)))\n",
    "print(\"==> Number of classes : {}\".format(num_class))\n",
    "print(\"==> Dimension of node features : {}\".format(feat_dim))\n",
    "print(\"==> Dimension of node state : {}\".format(stat_dim))\n",
    "print(\"==> T : {}\".format(T))\n",
    "print(\"==> Shape of feat_Matrix : {}\".format(feat_Matrix.shape))\n",
    "print(\"==> Shape of X_Node : {}\".format(X_Node.shape))\n",
    "print(\"==> Shape of X_Neis : {}\".format(X_Neis.shape))\n",
    "print(\"==> Length of dg_list : {}\".format(len(dg_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "实现论文中的Xi函数，作为Hw函数的转换矩阵A，根据节点对(i,j)的特征向量\n",
    "生成A矩阵，其中ln是特征向量维度，s为状态向量维度。\n",
    "Initialization :\n",
    "Input :\n",
    "    ln : (int)特征向量维度\n",
    "    s : (int)状态向量维度\n",
    "Forward :\n",
    "Input :\n",
    "    x : (Tensor)节点对(i,j)的特征向量拼接起来，shape为(N, 2*ln)\n",
    "Output :\n",
    "    out : (Tensor)A矩阵，shape为(N, s, s)\n",
    "'''\n",
    "class Xi(nn.Module):\n",
    "    def __init__(self, ln, s):\n",
    "        super(Xi, self).__init__()\n",
    "        self.ln = ln   # 节点特征向量的维度\n",
    "        self.s = s     # 节点的个数\n",
    "        \n",
    "        # 线性网络层\n",
    "        self.linear = nn.Linear(in_features=2 * ln,\n",
    "                                out_features=s ** 2,\n",
    "                                bias=True)\n",
    "        # 激活函数\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        bs = X.size()[0]\n",
    "        out = self.linear(X)\n",
    "        out = self.tanh(out)\n",
    "        return out.view(bs, self.s, self.s)\n",
    "\n",
    "\n",
    "'''\n",
    "实现论文中的Rou函数，作为Hw函数的偏置项b\n",
    "Initialization :\n",
    "Input :\n",
    "    ln : (int)特征向量维度\n",
    "    s : (int)状态向量维度\n",
    "Forward :\n",
    "Input :\n",
    "    x : (Tensor)节点的特征向量矩阵，shape(N, ln)\n",
    "Output :\n",
    "    out : (Tensor)偏置矩阵，shape(N, s)\n",
    "'''\n",
    "class Rou(nn.Module):\n",
    "    def __init__(self, ln, s):\n",
    "        super(Rou, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=ln,\n",
    "                                out_features=s,\n",
    "                                bias=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self, X):\n",
    "        return self.tanh(self.linear(X))\n",
    "\n",
    "'''\n",
    "实现Hw函数，即信息生成函数\n",
    "Initialize :\n",
    "Input :\n",
    "    ln : (int)节点特征向量维度\n",
    "    s : (int)节点状态向量维度\n",
    "    mu : (int)设定的压缩映射的压缩系数\n",
    "Forward :\n",
    "Input :\n",
    "    X : (Tensor)每一行为一条边的两个节点特征向量连接起来得到的向量，shape为(N, 2*ln)\n",
    "    H : (Tensor)与X每行对应的source节点的状态向量\n",
    "    dg_list : (list or Tensor)与X每行对应的source节点的度向量\n",
    "Output :\n",
    "    out : (Tensor)Hw函数的输出\n",
    "'''\n",
    "class Hw(nn.Module):\n",
    "    def __init__(self, ln, s, mu=0.9):\n",
    "        super(Hw, self).__init__()\n",
    "        self.ln = ln\n",
    "        self.s = s\n",
    "        self.mu = mu\n",
    "        \n",
    "        # 初始化网络层\n",
    "        self.Xi = Xi(ln, s)\n",
    "        self.Rou = Rou(ln, s)\n",
    "    \n",
    "    def forward(self, X, H, dg_list):\n",
    "        if isinstance(dg_list, list) or isinstance(dg_list, np.ndarray):\n",
    "            dg_list = torch.Tensor(dg_list).to(X.device)\n",
    "        elif isinstance(dg_list, torch.Tensor):\n",
    "            pass\n",
    "        else:\n",
    "            raise TypeError(\"==> dg_list should be list or tensor, not {}\".format(type(dg_list)))\n",
    "        A = (self.Xi(X) * self.mu / self.s) / dg_list.view(-1, 1, 1)# (N, S, S)\n",
    "        b = self.Rou(torch.chunk(X, chunks=2, dim=1)[0])# (N, S)\n",
    "        out = torch.squeeze(torch.matmul(A, torch.unsqueeze(H, 2)),-1) + b  # (N, s, s) * (N, s) + (N, s)\n",
    "        return out    # (N, s)\n",
    "\n",
    "'''\n",
    "实现信息聚合函数，将前面使用Hw函数得到的信息按照每一个source节点进行聚合，\n",
    "之后用于更新每一个节点的状态向量。\n",
    "Initialize :\n",
    "Input :\n",
    "    node_num : (int)节点的数量\n",
    "Forward :\n",
    "Input :\n",
    "    H : (Tensor)Hw的输出，shape为(N, s)\n",
    "    X_node : (Tensor)H每一行对应source节点的索引，shape为(N, )\n",
    "Output :\n",
    "    out : (Tensor)求和式聚合之后的新的节点状态向量，shape为(V, s)，V为节点个数\n",
    "'''\n",
    "class AggrSum(nn.Module):\n",
    "    def __init__(self, node_num):\n",
    "        super(AggrSum, self).__init__()\n",
    "        self.V = node_num\n",
    "    \n",
    "    def forward(self, H, X_node):\n",
    "        # H : (N, s) -> (V, s)\n",
    "        # X_node : (N, )\n",
    "        mask = torch.stack([X_node] * self.V, 0)\n",
    "        mask = mask.float() - torch.unsqueeze(torch.arange(0,self.V).float(), 1)\n",
    "        mask = (mask == 0).float()\n",
    "        # (V, N) * (N, s) -> (V, s)\n",
    "        return torch.mm(mask, H)\n",
    "\n",
    "'''\n",
    "实现Linear GNN模型，循环迭代计算T次，达到不动点之后，使用线性函数得到输出，进行\n",
    "分类。\n",
    "Initialize :\n",
    "Input :\n",
    "    node_num : (int)节点个数\n",
    "    feat_dim : (int)节点特征向量维度\n",
    "    stat_dim : (int)节点状态向量维度\n",
    "    T : (int)迭代计算的次数\n",
    "Forward :\n",
    "Input :\n",
    "    feat_Matrix : (Tensor)节点的特征矩阵，shape为(V, ln)\n",
    "    X_Node : (Tensor)每条边的source节点对应的索引，shape为(N, )，比如`节点i->节点j`，source节点是`节点i`\n",
    "    X_Neis : (Tensor)每条边的target节点对应的索引，shape为(N, )，比如`节点i->节点j`，target节点是`节点j`\n",
    "    dg_list : (list or Tensor)与X_Node对应节点的度列表，shape为(N, )\n",
    "Output :\n",
    "    out : (Tensor)每个节点的类别概率，shape为(V, num_class)\n",
    "'''\n",
    "class OriLinearGNN(nn.Module):\n",
    "    def __init__(self, node_num, feat_dim, stat_dim, num_class, T):\n",
    "        super(OriLinearGNN, self).__init__()\n",
    "        self.embed_dim = feat_dim\n",
    "        self.stat_dim = stat_dim\n",
    "        self.T = T\n",
    "        # 输出层\n",
    "        '''\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Linear(stat_dim, 16),   # ln+s -> hidden_layer\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(16, num_class)   # hidden_layer -> logits\n",
    "        )\n",
    "        '''\n",
    "        self.out_layer = nn.Linear(stat_dim, num_class)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "        # 实现Fw\n",
    "        self.Hw = Hw(feat_dim, stat_dim)\n",
    "        # 实现H的分组求和\n",
    "        self.Aggr = AggrSum(node_num)\n",
    "        \n",
    "    def forward(self, feat_Matrix, X_Node, X_Neis, dg_list):\n",
    "        node_embeds = torch.index_select(input=feat_Matrix,\n",
    "                                         dim=0,\n",
    "                                         index=X_Node)  # (N, ln)\n",
    "        neis_embeds = torch.index_select(input=feat_Matrix,\n",
    "                                         dim=0,\n",
    "                                         index=X_Neis)  # (N, ln)\n",
    "        X = torch.cat((node_embeds, neis_embeds), 1)    # (N, 2 * ln)\n",
    "        H = torch.zeros((feat_Matrix.shape[0], self.stat_dim), dtype=torch.float32)  # (V, s)\n",
    "        H = H.to(feat_Matrix.device)\n",
    "        # 循环T次计算\n",
    "        for t in range(self.T):\n",
    "            # (V, s) -> (N, s)\n",
    "            H = torch.index_select(H, 0, X_Neis)\n",
    "            # (N, s) -> (N, s)\n",
    "            H = self.Hw(X, H, dg_list)\n",
    "            # (N, s) -> (V, s)\n",
    "            H = self.Aggr(H, X_Node)\n",
    "            # print(H[1])\n",
    "        # out = torch.cat((feat_Matrix, H), 1)   # (V, ln+s)\n",
    "        out = self.log_softmax(self.dropout(self.out_layer(H)))\n",
    "        return out  # (V, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:116: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] Loss 2.0621, train acc 0.1148\n",
      "[Epoch 1/200] Loss 1.7113, train acc 0.3595\n",
      "[Epoch 2/200] Loss 1.4200, train acc 0.4649\n",
      "[Epoch 3/200] Loss 1.2635, train acc 0.5386\n",
      "[Epoch 4/200] Loss 1.1407, train acc 0.5580\n",
      "[Epoch 5/200] Loss 1.1056, train acc 0.5580\n",
      "[Epoch 6/200] Loss 1.0085, train acc 0.5861\n",
      "[Epoch 7/200] Loss 0.9609, train acc 0.5872\n",
      "[Epoch 8/200] Loss 0.8941, train acc 0.6060\n",
      "[Epoch 9/200] Loss 0.8718, train acc 0.5984\n",
      "Accuracy: 0.7320\n",
      "[Epoch 10/200] Loss 0.8575, train acc 0.6054\n",
      "[Epoch 11/200] Loss 0.8098, train acc 0.6230\n",
      "[Epoch 12/200] Loss 0.7966, train acc 0.6189\n",
      "[Epoch 13/200] Loss 0.7995, train acc 0.6189\n",
      "[Epoch 14/200] Loss 0.8306, train acc 0.6036\n",
      "[Epoch 15/200] Loss 0.7850, train acc 0.6153\n",
      "[Epoch 16/200] Loss 0.7772, train acc 0.6171\n",
      "[Epoch 17/200] Loss 0.7668, train acc 0.6230\n",
      "[Epoch 18/200] Loss 0.7535, train acc 0.6317\n",
      "[Epoch 19/200] Loss 0.7471, train acc 0.6341\n",
      "Accuracy: 0.7780\n",
      "[Epoch 20/200] Loss 0.7302, train acc 0.6382\n",
      "[Epoch 21/200] Loss 0.7440, train acc 0.6265\n",
      "[Epoch 22/200] Loss 0.7627, train acc 0.6200\n",
      "[Epoch 23/200] Loss 0.7514, train acc 0.6118\n",
      "[Epoch 24/200] Loss 0.6879, train acc 0.6511\n",
      "[Epoch 25/200] Loss 0.7002, train acc 0.6434\n",
      "[Epoch 26/200] Loss 0.7318, train acc 0.6352\n",
      "[Epoch 27/200] Loss 0.7097, train acc 0.6376\n",
      "[Epoch 28/200] Loss 0.7018, train acc 0.6481\n",
      "[Epoch 29/200] Loss 0.7132, train acc 0.6329\n",
      "Accuracy: 0.7740\n",
      "[Epoch 30/200] Loss 0.7323, train acc 0.6294\n",
      "[Epoch 31/200] Loss 0.7153, train acc 0.6382\n",
      "[Epoch 32/200] Loss 0.7424, train acc 0.6177\n",
      "[Epoch 33/200] Loss 0.6917, train acc 0.6434\n",
      "[Epoch 34/200] Loss 0.7220, train acc 0.6171\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-983af19e6ed0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Get output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat_Matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_Node\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_Neis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdg_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Get loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-604e128bf435>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, feat_Matrix, X_Node, X_Neis, dg_list)\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdg_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;31m# (N, s) -> (V, s)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAggr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_Node\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[1;31m# print(H[1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;31m# out = torch.cat((feat_Matrix, H), 1)   # (V, ln+s)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-604e128bf435>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, H, X_node)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m# H : (N, s) -> (V, s)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# X_node : (N, )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_node\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "train_mask = torch.zeros(node_num, dtype=torch.bool)\n",
    "train_mask[:node_num - 1000] = 1                  # 1700左右training\n",
    "val_mask = None                                    # 0valid\n",
    "test_mask = torch.zeros(node_num, dtype=torch.bool)\n",
    "test_mask[node_num - 500:] = 1                    # 500test\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = OriLinearGNN(node_num, feat_dim, stat_dim, num_class, T).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-3)\n",
    "feat_Matrix = feat_Matrix.to(device)\n",
    "X_Node = X_Node.to(device)\n",
    "X_Neis = X_Neis.to(device)\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get output\n",
    "    out = model(feat_Matrix, X_Node, X_Neis, dg_list)\n",
    "    \n",
    "    # Get loss\n",
    "    loss = F.nll_loss(out[train_mask], label_list[train_mask])\n",
    "    _, pred = out.max(dim=1)\n",
    "    \n",
    "    # Get predictions and calculate training accuracy\n",
    "    correct = float(pred[train_mask].eq(label_list[train_mask]).sum().item())\n",
    "    acc = correct / train_mask.sum().item()\n",
    "    print('[Epoch {}/200] Loss {:.4f}, train acc {:.4f}'.format(epoch, loss.cpu().detach().data.item(), acc))\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluation on test data every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        model.eval()\n",
    "        _, pred = model(feat_Matrix, X_Node, X_Neis, dg_list).max(dim=1)\n",
    "        correct = float(pred[test_mask].eq(label_list[test_mask]).sum().item())\n",
    "        acc = correct / test_mask.sum().item()\n",
    "        print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
