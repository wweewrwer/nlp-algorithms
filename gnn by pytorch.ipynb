{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用原生pytorch实现最原生的linear GNN模型，论文为2009年的[《The graph neural network model》](https://persagen.com/files/misc/scarselli2009graph.pdf)的GNN,\n",
    "代码部分大幅借鉴了[Github用户LYuhang](https://github.com/LYuhang/GNN_Review/blob/master/PyG%E5%92%8CPytorch%E5%AE%9E%E7%8E%B0GNN%E6%A8%A1%E5%9E%8B/GNN_Implement_with_Pytorch.ipynb),建议大家以他的讲解为主，我的作为一些细节的补充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "#Tqdm 是一个快速，可扩展的Python进度条，可以在 Python 长循环中添加一个进度提示信息，用户只需要封装任意的迭代器 tqdm(iterator)\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cpu'\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用的数据集为[cora数据集](https://linqs.soe.ucsc.edu/data)(该网站还有其它关于图神经网络的数据集),该数据集由许多机器学习领域的paper构成，这些paper被分为7个类别，在该数据集中，一篇论文至少与该数据集中任一其它论文有引用或被引用关系，共2708篇论文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总共包含两个文件：  \n",
    "1.`cora.content`文件包含对paper的内容描述，格式为$$ \\text{<paper_id> <word_attributes> <class_label>} $$其中：  \n",
    "&emsp;`<paper_id>`是paper的标识符，每一篇paper对应一个标识符。  \n",
    "&emsp;`<word_attributes>`是词汇特征，为0或1，表示对应词汇是否存在。  \n",
    "&emsp;`<class_label>`是该文档所述的类别。  \n",
    "  \n",
    "2.`cora.cites`包含了数据集的引用图，格式为$$ \\text{<ID of cited paper> <ID of citing paper>} $$其中：  \n",
    "&emsp;`<ID of cited paper>`是被引用的paper标识符。  \n",
    "&emsp;`<ID of citing paper>`是引用的paper标识符。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "node_num, feat_dim, stat_dim, num_class, T\n",
    "feat_Matrix, X_Node, X_Neis, dg_list\n",
    "'''\n",
    "#数据处理\n",
    "content_path = \"./cora/cora.content\"\n",
    "cite_path = \"./cora/cora.cites\"\n",
    "\n",
    "#读取文本内容\n",
    "with open(content_path, \"r\") as fp:\n",
    "    contents = fp.readlines()\n",
    "with open(cite_path, \"r\") as fp:\n",
    "    cites = fp.readlines()\n",
    "    \n",
    "contents = np.array([np.array(s.strip().split(\"\\t\")) for s in contents])\n",
    "paper_list, feat_list, label_list = np.split(contents, [1,-1], axis=1)\n",
    "paper_list, label_list = np.squeeze(paper_list), np.squeeze(label_list)\n",
    "\n",
    "# paper -> index dict\n",
    "#print(\"paper_list\",sorted(paper_list))\n",
    "paper_dict = dict([(key, val) for val, key in enumerate(paper_list)])\n",
    "\n",
    "# lable -> index dict\n",
    "labels = list(set(label_list))\n",
    "label_dict = dict([(key, val) for val, key in enumerate(labels)])\n",
    "\n",
    "# edge_index\n",
    "cites = [i.strip().split(\"\\t\") for i in cites]\n",
    "#print(cites)\n",
    "#下面这几句代码不一样\n",
    "#print(paper_dict)\n",
    "cites = np.array([[paper_dict[i[0]], paper_dict[i[1]]] for i in cites], dtype = np.int64)\n",
    "#print(cites[1:7])\n",
    "cites = np.concatenate((cites, cites[:, ::-1]), axis=0) \n",
    "#print(cites[1:7])\n",
    "#这句也不一样\n",
    "#print(cites[:,0])\n",
    "degree_list=np.zeros(len(paper_list), dtype = np.int32)\n",
    "for i in cites:\n",
    "    degree_list[i[0]] += 1\n",
    "#_, degree_list = np.unique(cites[:,0],return_counts=True)\n",
    "#print(degree_list)\n",
    "\n",
    "#input\n",
    "node_num = len(paper_list)\n",
    "feat_dim = feat_list.shape[1]\n",
    "stat_dim = 32\n",
    "num_class = len(labels)\n",
    "T = 2\n",
    "feat_Matrix = torch.Tensor(feat_list.astype(np.float32))\n",
    "X_Node, X_Neis = np.split(cites, 2, axis=1)\n",
    "X_Node, X_Neis = torch.tensor(np.squeeze(X_Node)), \\\n",
    "                 torch.tensor(np.squeeze(X_Neis))\n",
    "#print(X_Node)\n",
    "#print(degree_list[163])\n",
    "dg_list = torch.tensor(degree_list[X_Node])\n",
    "label_list = np.array([label_dict[i] for i in label_list])\n",
    "label_list = torch.tensor(label_list, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of node :  2708\n",
      "Number of edges :  10858\n",
      "Number of classes :  7\n",
      "Dimension of node features :  1433\n",
      "Dimension of node state :  32\n",
      "Shape of feat_Matrix :  torch.Size([2708, 1433])\n",
      "Shape of X_Node :  torch.Size([10858])\n",
      "Shape of X_Neis :  torch.Size([10858])\n",
      "Length of dg_list :  10858\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of node : \", node_num)\n",
    "print(\"Number of edges : \", cites.shape[0])\n",
    "print(\"Number of classes : \", num_class)\n",
    "print(\"Dimension of node features : \", feat_dim)\n",
    "print(\"Dimension of node state : \", stat_dim)\n",
    "print(\"Shape of feat_Matrix : \", feat_Matrix.shape)\n",
    "print(\"Shape of X_Node : \", X_Node.shape)\n",
    "print(\"Shape of X_Neis : \", X_Neis.shape)\n",
    "print(\"Length of dg_list : \", len(dg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](equation1.jpg)\n",
    "![avatar](equation2.jpg)\n",
    "![avatar](equation3.jpg)\n",
    "其中，关于各参数的解释见论文第10页,以下附上截图：  \n",
    "其中s是state的维度\n",
    "![avatar](linearGNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "下面Xi函数是生成论文中A矩阵方程式中的Xi矩阵(就是符号很奇怪的那个矩阵）。\n",
    "然后我的phi函数是直接把ln,lu拼接起来，\n",
    "l(n,u)没有（其实这里应该加上更好，引用边和被引用边显然应该用不同的label）\n",
    "Initialization :\n",
    "Input :\n",
    "    ln : (int)特征向量维度\n",
    "    s : (int)状态向量维度\n",
    "Forward : \n",
    "N为节点数\n",
    "Input :\n",
    "    input : (Tensor)节点对(i,j)的特征向量拼接起来，shape为(N，2*ln)\n",
    "Output :\n",
    "    out : (Tensor)Xi矩阵，shape为(N, s, s)\n",
    "'''\n",
    "class Xi(nn.Module):\n",
    "    def __init__(self, ln, s):\n",
    "        super(Xi, self).__init__()\n",
    "        self.ln = ln\n",
    "        self.s = s\n",
    "        \n",
    "        self.linear = nn.Linear(2 * ln, s ** 2)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        #在jupyter中用F.tanh会有警告，它会让你用torch.tanh\n",
    "        #它的警告是F.tanh is deprecated,但是我网上都没有查到这个\n",
    "        #不过反正torch.tanh是一样的，那就不用F.tanh了\n",
    "        output = torch.tanh(self.linear(input))\n",
    "        return output.view(-1, self.s, self.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5258, -0.4721,  0.1365],\n",
      "         [-0.1282,  0.3701,  0.3710],\n",
      "         [-0.0063,  0.2418,  0.9394]],\n",
      "\n",
      "        [[-0.2679, -0.5961, -0.0842],\n",
      "         [-0.0535, -0.0468,  0.3340],\n",
      "         [ 0.2911, -0.1686,  0.0429]],\n",
      "\n",
      "        [[-0.0015, -0.4276, -0.3208],\n",
      "         [-0.4746, -0.6045,  0.1372],\n",
      "         [-0.2700, -0.3734,  0.3752]],\n",
      "\n",
      "        [[-0.7382, -0.3882, -0.3907],\n",
      "         [-0.3630, -0.3021,  0.1204],\n",
      "         [-0.6484, -0.5026, -0.1991]]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "#测试正确与否：\n",
    "model1 = Xi(5,3)\n",
    "input1 = torch.randn(4,10)\n",
    "print(model1(input1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "实现论文中rou矩阵，并生成偏置项b\n",
    "Initialization :\n",
    "Input :\n",
    "    ln : (int)特征向量维度\n",
    "    s : (int)状态向量维度\n",
    "Forward :\n",
    "Input :\n",
    "    input : (Tensor)节点的特征向量矩阵，shape(N, ln)\n",
    "Output :\n",
    "    out : (Tensor)偏置矩阵，shape(N, s)\n",
    "'''\n",
    "class Rou(nn.Module):\n",
    "    def __init__(self, ln, s):\n",
    "        super(Rou, self).__init__()\n",
    "        self.linear = nn.Linear(ln,s)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return torch.tanh(self.linear(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1042, -0.7392, -0.1989],\n",
      "        [ 0.1593, -0.7287, -0.2371],\n",
      "        [ 0.6952,  0.1904,  0.2597],\n",
      "        [-0.3671,  0.1082,  0.2092]], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "model1 = Rou(5,3)\n",
    "input1 = torch.randn(4,5)\n",
    "print(model1(input1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "实现Hw函数，就是论文中的公式12,其中|ne[u]|的意思是ne[u]邻居的个数\n",
    "Initialize :\n",
    "Input :\n",
    "    ln : (int)节点特征向量维度\n",
    "    s : (int)节点状态向量维度\n",
    "    mu : (int)设定的压缩映射的压缩系数\n",
    "Forward :\n",
    "Input :\n",
    "    X : (Tensor)每一行为一条边的两个节点特征向量连接起来得到的向量，shape为(N, 2*ln)\n",
    "    H : (Tensor)与X每行对应u=one of ne[n](即x中的第二个节点,这里默认边是第二个点连到第一个，即2是1的ne,但1不是2的ne)的状态向量\n",
    "    dg_list: (Tensor)与X每行对应u=one of ne[n]的度数向量ne[u]\n",
    "Output :\n",
    "    out : (Tensor)Hw函数的输出\n",
    "'''\n",
    "class Hw(nn.Module):\n",
    "    def __init__(self, ln, s, mu=0.9):\n",
    "        super(Hw, self).__init__()\n",
    "        self.ln = ln\n",
    "        self.s = s\n",
    "        self.mu = mu\n",
    "        \n",
    "        self.Xi = Xi(ln, s)\n",
    "        self.Rou = Rou(ln, s)\n",
    "    \n",
    "    '''\n",
    "    A: N * s * s\n",
    "    b: N * s\n",
    "    '''\n",
    "    def forward(self, X, H, dg_list):\n",
    "        A = (self.Xi(X) * self.mu / self.s)/dg_list.view(-1, 1, 1)\n",
    "        b = self.Rou(torch.chunk(X, 2, dim=1)[0])#chunk函数：分割tensor，(n,ln)->(n,s)\n",
    "        #下面必须这样先unsqueeze再squeeze,可以参考matmul规则\n",
    "        #matmul高维矩阵相乘：自动在前补1*，使维数相同，然后广播，使高于最后两维的size相同\n",
    "        #然后依次对里面进行二维的矩阵乘法\n",
    "        #如(j*1*n*p)(k*p*m)->(j*k*n*p)(j*k*p*m)->(j*k*n*m) \n",
    "        output = torch.squeeze(torch.matmul(A, torch.unsqueeze(H,2)),-1) + b\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5858, -0.3022,  0.0928],\n",
      "        [ 0.3533,  0.6297, -0.3358],\n",
      "        [-0.4971,  0.0896, -0.1976],\n",
      "        [-0.0315,  0.3390,  0.0085]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model1 = Hw(5,3)\n",
    "X1 = torch.randn(4,10)\n",
    "H1 = torch.randn(4,3)\n",
    "dg_list1 = torch.tensor([2,3,4,5])\n",
    "print(model1(X1, H1, dg_list1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "怎么相加呢？这是很多地方都没有提及的。for循环一个个访问它的邻接点吗？本身这种方法也可行，但是很多实现方法不是这样的，而是：\n",
    "$$X = EH \\quad (1)$$\n",
    "我们把所有边的H叠到一起，假设总共e条边，然后我们把每条边需要更新的节点也列成一个列表，长度为e，然后假设n个点，我这个列表复制n份叠成矩阵，然后对于拿去更新i节点的那一行就通通减去i,然后令这一行中=0的统统为1，其它为0，就得到了那个方程（3）中的E矩阵，然后用E*H就更新了，这里的矩阵乘法就是这样来的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "实现上面论文中的方程（3），将前面使用的hw函数得到的信息相加了，\n",
    "并更新每一个节点的状态向量\n",
    "Initialize :\n",
    "Input :\n",
    "    node_num : (int)节点的数量\n",
    "Forward :\n",
    "Input :\n",
    "    H : (Tensor)Hw的输出，shape为(E, s)\n",
    "    X_node : (Tensor)H每一行对应要更新的节点的索引(E)\n",
    "Output :\n",
    "    out :(Tensor)新的节点状态向量，shape为(N, s), N为节点个数\n",
    "'''\n",
    "class AggrSum(nn.Module):\n",
    "    def __init__(self, node_num):\n",
    "        super(AggrSum, self).__init__()\n",
    "        self.N = node_num\n",
    "        \n",
    "    def forward(self, H, X_node):\n",
    "        #H : (E, s) -> (N,s)\n",
    "        #感觉写出矩阵乘法有一点点浪费时间和空间\n",
    "        #但是for循环寻址也挺慢的,方便来看还是写矩阵乘法吧\n",
    "        mask = torch.stack([X_node] * self.N, 0)\n",
    "        mask = mask.float() - torch.unsqueeze(torch.arange(0,self.N).float(), 1)\n",
    "        mask = (mask == 0).float()\n",
    "        # (V, N) * (N, s) -> (V, s)\n",
    "        return torch.mm(mask, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9408, -0.2728, -1.4347],\n",
      "        [ 0.7895,  0.0469, -0.5483]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "model1 = Hw(5,3)\n",
    "X1 = torch.randn(4,10)\n",
    "H1 = torch.randn(4,3)\n",
    "dg_list1 = torch.tensor([2,3,4,5])\n",
    "model2 = AggrSum(2)\n",
    "print(model2(model1(X1, H1, dg_list1), torch.tensor([0,1,1,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "实现Linear GNN模型，循环迭代计算T次，\n",
    "达到不动点之后，使用线性函数得到输出，进行分类\n",
    "Initialize : \n",
    "Input :\n",
    "    node_num : (int)节点个数\n",
    "    feat_dim : (int)节点特征向量维度\n",
    "    stat_dim : (int)节点状态向量维度\n",
    "    T : (int)迭代计算的次数\n",
    "Forward :\n",
    "Input :\n",
    "    feat_Matrix : (Tensor)节点的特征矩阵，shape为（N，ln)\n",
    "    X_Node : (Tensor)每条边的提供更新的节点的索引，例如i->j,i就是提供更新的，（N）\n",
    "    X_Neis : (Tensor)每条边被更新的节点的索引，（N）\n",
    "    dg_list : (Tensor)与X_Node对应节点的度列表，shape为（N）\n",
    "    out : (Tensor)每个节点的类别概率，shape为（V，num_class)\n",
    "'''\n",
    "class OriLinearGNN(nn.Module):\n",
    "    def __init__(self, node_num, feat_dim, stat_dim, num_class, T):\n",
    "        super(OriLinearGNN, self).__init__()\n",
    "        self.embed_dim = feat_dim\n",
    "        self.stat_dim = stat_dim\n",
    "        self.T = T\n",
    "        \n",
    "        self.out_layer = nn.Linear(stat_dim, num_class)\n",
    "        #这里天坑，由于不需要学习的我一向喜欢直接用F.而不是类\n",
    "        #结果之前一直准确率不对，才发现如果用的是类在预测时会自动关闭的，而函数则不会\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        self.Hw = Hw(feat_dim, stat_dim)\n",
    "        self.Aggr = AggrSum(node_num)\n",
    "        \n",
    "    def forward(self, feat_Matrix, X_Node, X_Neis, dg_list):\n",
    "        #这里是取出要用到的特征向量\n",
    "        node_embeds = torch.index_select(feat_Matrix, 0, X_Node)\n",
    "        neis_embeds = torch.index_select(feat_Matrix, 0, X_Neis)\n",
    "        X = torch.cat((node_embeds, neis_embeds), 1) #N, 2*ln\n",
    "        #H是其中的状态向量\n",
    "        H = torch.zeros((feat_Matrix.shape[0], self.stat_dim), dtype = torch.float32).to(device)\n",
    "        for t in range(self.T):\n",
    "            #(V, s) -> (N, s)\n",
    "            #这里是取出要用到的状态向量\n",
    "            H = torch.index_select(H, 0, X_Neis)\n",
    "            #(N, s) -> (N, s)\n",
    "            #这里是得到能拿来更新状态向量的矩阵\n",
    "            H = self.Hw(X, H, dg_list)\n",
    "            #(N, s) -> (V, s)\n",
    "            #这里是更新状态向量\n",
    "            H = self.Aggr(H, X_Node)\n",
    "        output = F.log_softmax(self.dropout(self.out_layer(H)),dim = -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] Loss 2.0806, train acc 0.1136\n",
      "[Epoch 1/200] Loss 1.7029, train acc 0.3788\n",
      "[Epoch 2/200] Loss 1.4028, train acc 0.4877\n",
      "[Epoch 3/200] Loss 1.2574, train acc 0.5732\n",
      "[Epoch 4/200] Loss 1.0947, train acc 0.6112\n",
      "[Epoch 5/200] Loss 1.0061, train acc 0.6505\n",
      "[Epoch 6/200] Loss 0.9552, train acc 0.6464\n",
      "[Epoch 7/200] Loss 0.9074, train acc 0.6733\n",
      "[Epoch 8/200] Loss 0.8408, train acc 0.6774\n",
      "[Epoch 9/200] Loss 0.8791, train acc 0.6721\n",
      "Accuracy: 0.7660\n",
      "[Epoch 10/200] Loss 0.8563, train acc 0.7014\n",
      "[Epoch 11/200] Loss 0.7866, train acc 0.6874\n",
      "[Epoch 12/200] Loss 0.7994, train acc 0.6756\n",
      "[Epoch 13/200] Loss 0.7954, train acc 0.6803\n",
      "[Epoch 14/200] Loss 0.7697, train acc 0.6985\n",
      "[Epoch 15/200] Loss 0.7687, train acc 0.6944\n",
      "[Epoch 16/200] Loss 0.7942, train acc 0.6979\n",
      "[Epoch 17/200] Loss 0.7479, train acc 0.6833\n",
      "[Epoch 18/200] Loss 0.7324, train acc 0.6868\n",
      "[Epoch 19/200] Loss 0.7569, train acc 0.6821\n",
      "Accuracy: 0.7620\n"
     ]
    }
   ],
   "source": [
    "#split dataset\n",
    "train_mask = torch.zeros(node_num, dtype = torch.bool)\n",
    "train_mask[:node_num - 1000] = 1               #1700左右training\n",
    "\n",
    "val_mask = None                                \n",
    "test_mask = torch.zeros(node_num, dtype = torch.bool)\n",
    "test_mask[node_num - 500:] = 1                 # 500test\n",
    "\n",
    "model = OriLinearGNN(node_num, feat_dim, stat_dim, num_class, T).to(device)\n",
    "\n",
    "#Adam是一种算法，可以百度了解\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01, weight_decay = 1e-3)\n",
    "feat_Matrix = feat_Matrix.to(device)\n",
    "X_Node = X_Node.to(device)\n",
    "X_Neis = X_Neis.to(device)\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(feat_Matrix, X_Node, X_Neis, dg_list)\n",
    "    \n",
    "    loss = F.nll_loss(out[train_mask], label_list[train_mask])\n",
    "    _, pred = out.max(dim=1)\n",
    "    \n",
    "    correct = float(pred[train_mask].eq(label_list[train_mask]).sum().item())\n",
    "    acc = correct / train_mask.sum().item()\n",
    "    print('[Epoch {}/200] Loss {:.4f}, train acc {:.4f}'.format(epoch, loss.cpu().detach().data.item(), acc))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        _, pred = model(feat_Matrix, X_Node, X_Neis, dg_list).max(dim = 1)\n",
    "        correct = float(pred[test_mask].eq(label_list[test_mask]).sum().item())\n",
    "        acc = correct / test_mask.sum().item()\n",
    "        print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
