{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此文建立在最原始的linear GNN已经理解的基础，讲解。\n",
    "参考资料：https://zhuanlan.zhihu.com/p/38612863\n",
    "\n",
    "首先，GCN相对于GNN哪些地方变了？为什么变了？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么首先我们来看GNN的全过程： \n",
    "$$X = F(X,l) \\quad (0)$$\n",
    "$$X = EH \\quad (1)$$\n",
    "$$O = G(X,l) \\quad (2)$$\n",
    "![avatar](equation1.jpg)\n",
    "![avatar](equation2.jpg)\n",
    "![avatar](equation3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是关于这些公式的解释:  \n",
    "首先是参数解释：  \n",
    "不带下标表示总的，带下标表示某个。  \n",
    "&emsp;X：节点的状态向量  \n",
    "&emsp;l：节点或边的特征向量  \n",
    "&emsp;h：更新过程中每条边产生的，用来更新状态向量的，堆叠而成的  \n",
    "其它的变量可以参考论文，它们在这里不重要  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是这几个方程的解释：  \n",
    "首先(0),(2)是GNN最概括的表述，F函数用于更新，更新T次到达不动点后用G函数输出。那么这两个函数是怎么样的呢？  \n",
    "先说（2），（2)比较容易实现，(2)直接根据你的任务用最简单的方式实现就行了，比如直接一个前馈式神经网络用一个`nn.Linear`，把X放进去吐出来就行了，当然，也可以把l也放进去，拼接一下或者分别用`nn.Linear`把维度弄相同叠加也可以  \n",
    "然后（0），（0）的处理即是（3），把每条边用H函数来处理，然后更新，h函数的具体实现参考论文即可了，或者参考前面我的gnn的讲解  \n",
    "但是那么方程（1）呢？这就是网上很多讲解GCN和GNN没有提及的。这是代码和论文中没有提及的地方。方程（3）如何代码实现呢？for循环一个个访问它的邻接点吗？本身这种方法也可行，但是很多实现方法不是这样的，而是：我们把所有边的H叠到一起，假设总共e条边，然后我们把每条边需要更新的节点也列成一个列表，长度为e，然后假设n个点，我这个列表复制n份叠成矩阵，然后对于拿去更新i节点的那一行就通通减去i,然后令这一行中=0的统统为1，其它为0，就得到了那个方程（3）中的E矩阵，然后用E\\*H就更新了，这里的矩阵乘法就是这样来的,这种思想，在接下来的GCN也有所体现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么现在GCN动了GNN的哪些地方呢？\n",
    "其实改动特别大，主要动的便是方程（12）的实现。首先我们先假设方程（12）为\n",
    "$$h_u=X_uW$$\n",
    "那么我们可以发现，结合GNN用矩阵来代替for的思路，我们可以得到：\n",
    "$$X^{t+1}=AX^{t}W$$\n",
    "其中A为临接矩阵，可以XW就可以得到所有节点产生的用于更新的信息，再乘A就可以了。  \n",
    "但是这样有问题。因为对于任一一个节点来说，本身的信息是很重要的，这个居然完全放弃了本身t时刻的状态向量。  \n",
    "所以令A=A+I（I为单位矩阵），这样就补上了。\n",
    "但是还有一个问题：A没有被规范化，这样是有问题的，比如相邻节点多的更新就明显比其它的大。  \n",
    "那么问题就在于如何规范化A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个时候就要引入卷积了。直接来看推导的结果（推导过程可以自行参考[网上的教程](http://xtf615.com/2019/02/24/gcn/)）这里仅记录理解的几个关键点：\n",
    "![avatar](gcn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中H表示状态向量，其中：\n",
    "$$\\tilde{A} = A + I$$\n",
    "$$\\tilde{D} = D + I$$\n",
    "其中D为度数矩阵，即对角线上点i的度数，其它全为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式推导的几个关键点：  \n",
    "1.由傅里叶级数到傅里叶变换时，其实纯数学的方法更好理解，直接在级数的公式上代入an,bn,sin,cos(前两个就是用f(x)来表示的积分式子，后两个就是欧拉公式用e来表示的式子，即可得到cn的一个表达式，在那个式子另y=n/T,然后T取无穷很简单，一下就可以想明白。  \n",
    "2.拉普拉斯算子那个教程讲的很详细了，离散的数据就是用近似的方法，可以结合导数定义，这样就好理解了。（本地名为laplace的截图就是这个）该博客中的D为度数矩阵，W为邻接矩阵，不带下标的f为所有点的值叠加起来的向量（矩阵）  \n",
    "3.线性代数知识复习：  \n",
    "半正定矩阵：设A是n阶实对称矩阵，如果对任何非零向量X，都有X'AX≥0，其中X'表示X的转置，就称A为半正定矩阵，正定就是是>0.    \n",
    "特征值与特征向量：设 Ａ 是 ｎ 阶矩阵，如果数 λ 和 ｎ 维非零列向量 α 使关系式Ａα＝λα成立，那么数 λ 称为矩阵 Ａ 的特征值，非零向量 α 称为 Ａ 的对应于特征值 λ 的特征向量  \n",
    "4.在类比时，特征函数就是由特征向量构成的函数，其任取一个值都可以作特征向量  \n",
    "5.先依次看推导，当不懂时，往下看，看它是要干什么，再反过来看。比如看不懂图上的傅里叶变换了，就往下看看为什么要用这个东西，发现是类比CNN为了求卷积。  \n",
    "6.现在学的GCN是第三代GCN，它的推导不是我上面说的那样，不过感觉可以这样理解。但是在理解为什么有卷积的时候，不要这样想，因为卷积并不是仅仅涉及ne[n],三代GCN的这个公式是简化来的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cpu'\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "node_num, feat_dim, stat_dim, num_class\n",
    "feat_Matrix, cites\n",
    "'''\n",
    "#数据处理\n",
    "content_path = \"./cora/cora.content\"\n",
    "cite_path = \"./cora/cora.cites\"\n",
    "\n",
    "#读取文本内容\n",
    "with open(content_path, \"r\") as fp:\n",
    "    contents = fp.readlines()\n",
    "with open(cite_path, \"r\") as fp:\n",
    "    cites = fp.readlines()\n",
    "    \n",
    "contents = np.array([np.array(s.strip().split(\"\\t\")) for s in contents])\n",
    "paper_list, feat_list, label_list = np.split(contents, [1,-1], axis=1)\n",
    "paper_list, label_list = np.squeeze(paper_list), np.squeeze(label_list)\n",
    "\n",
    "# paper -> index dict\n",
    "#print(\"paper_list\",sorted(paper_list))\n",
    "paper_dict = dict([(key, val) for val, key in enumerate(paper_list)])\n",
    "\n",
    "# lable -> index dict\n",
    "labels = list(set(label_list))\n",
    "label_dict = dict([(key, val) for val, key in enumerate(labels)])\n",
    "\n",
    "# edge_index\n",
    "cites = [i.strip().split(\"\\t\") for i in cites]\n",
    "cites = np.array([[paper_dict[i[0]], paper_dict[i[1]]] for i in cites], dtype = np.int64)\n",
    "node_num = len(paper_list)\n",
    "feat_dim = feat_list.shape[1]\n",
    "stat_dim = 128\n",
    "num_class = len(labels)\n",
    "feat_Matrix = torch.Tensor(feat_list.astype(np.float32))\n",
    "label_list = np.array([label_dict[i] for i in label_list])\n",
    "label_list = torch.tensor(label_list, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of node :  2708\n",
      "Number of edges :  5429\n",
      "Number of classes :  7\n",
      "Dimension of node features :  1433\n",
      "Dimension of node state :  128\n",
      "Shape of feat_Matrix :  torch.Size([2708, 1433])\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of node : \", node_num)\n",
    "print(\"Number of edges : \", cites.shape[0])\n",
    "print(\"Number of classes : \", num_class)\n",
    "print(\"Dimension of node features : \", feat_dim)\n",
    "print(\"Dimension of node state : \", stat_dim)\n",
    "print(\"Shape of feat_Matrix : \", feat_Matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNlayer(nn.Module):\n",
    "    def __init__(self, node_num, feat_num):\n",
    "        super(GCNlayer, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(feat_num,feat_num,bias=False)\n",
    "    \n",
    "    def forward(self, matDAD, H):\n",
    "        return F.relu(self.linear(torch.matmul(matDAD,H)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, node_num, feat_num, stat_num, num_class):\n",
    "        super(GCN,self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(feat_num, stat_num)\n",
    "        self.gcn_layer1 = GCNlayer(node_num, stat_num)\n",
    "        self.gcn_layer2 = GCNlayer(node_num, stat_num)\n",
    "        self.out_layer = nn.Linear(stat_num, num_class)\n",
    "        #这里天坑，由于不需要学习的我一向喜欢直接用F.而不是类\n",
    "        #结果之前一直准确率不对，才发现如果用的是类在预测时会自动关闭的，而函数则不会\n",
    "        self.dropout = nn.Dropout()\n",
    "    \n",
    "    def forward(self, matDAD, X):\n",
    "        H = self.input_layer(X)\n",
    "        H = self.gcn_layer1(matDAD, H)\n",
    "        H = self.gcn_layer2(matDAD, H)\n",
    "        output = F.log_softmax(self.dropout(self.out_layer(H)),dim = -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0395, -2.0395, -2.0395, -2.0395, -2.0027, -1.5392, -2.0395],\n",
      "        [-1.9801, -1.4591, -1.9801, -1.9801, -1.9801, -1.9801, -2.5606],\n",
      "        [-2.5477, -1.7843, -1.7843, -1.7843, -1.8548, -1.7843, -2.3689],\n",
      "        [-5.0380, -1.0789, -3.8959, -1.6337, -1.6337, -3.0446, -1.6337],\n",
      "        [-2.0685, -1.5571, -2.4845, -1.3874, -2.0685, -2.0685, -2.5634]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "model1 = GCN(5,7,3,7)\n",
    "matDAD1 = torch.randn(5,5)\n",
    "H1 = torch.randn(5,7)\n",
    "print(model1(matDAD1, H1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edgeToMat(edges, node_num):\n",
    "    A = torch.eye(node_num)\n",
    "    for i in edges:\n",
    "        A[i[0]][i[1]] += 1\n",
    "        A[i[1]][i[0]] += 1\n",
    "    D = torch.sum(A,dim=0)\n",
    "    D = torch.diag(torch.pow(D , -0.5))\n",
    "    return torch.matmul(torch.matmul(D,A),D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3333, 0.0000, 0.2887, 0.3333],\n",
      "        [0.0000, 0.5000, 0.3536, 0.0000],\n",
      "        [0.2887, 0.3536, 0.2500, 0.2887],\n",
      "        [0.3333, 0.0000, 0.2887, 0.3333]])\n"
     ]
    }
   ],
   "source": [
    "a1= [[0,2],[0,3],[1,2],[2,3]]\n",
    "print(edgeToMat(a1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] Loss 1.9496, train acc 0.1247\n",
      "[Epoch 1/200] Loss 1.9196, train acc 0.2061\n",
      "[Epoch 2/200] Loss 1.8057, train acc 0.2482\n",
      "[Epoch 3/200] Loss 1.6479, train acc 0.3519\n",
      "[Epoch 4/200] Loss 1.6107, train acc 0.3841\n",
      "[Epoch 5/200] Loss 1.4275, train acc 0.4063\n",
      "[Epoch 6/200] Loss 1.3452, train acc 0.4093\n",
      "[Epoch 7/200] Loss 1.2077, train acc 0.4660\n",
      "[Epoch 8/200] Loss 1.1683, train acc 0.4766\n",
      "[Epoch 9/200] Loss 1.1016, train acc 0.4977\n",
      "Accuracy: 0.7580\n",
      "[Epoch 10/200] Loss 1.0370, train acc 0.5246\n",
      "[Epoch 11/200] Loss 1.0045, train acc 0.5293\n",
      "[Epoch 12/200] Loss 0.9859, train acc 0.5263\n",
      "[Epoch 13/200] Loss 0.9747, train acc 0.5281\n",
      "[Epoch 14/200] Loss 0.8993, train acc 0.5527\n",
      "[Epoch 15/200] Loss 0.8643, train acc 0.5632\n",
      "[Epoch 16/200] Loss 0.8881, train acc 0.5433\n",
      "[Epoch 17/200] Loss 0.8324, train acc 0.5550\n",
      "[Epoch 18/200] Loss 0.8215, train acc 0.5708\n",
      "[Epoch 19/200] Loss 0.7826, train acc 0.5925\n",
      "Accuracy: 0.7920\n",
      "[Epoch 20/200] Loss 0.8109, train acc 0.5726\n",
      "[Epoch 21/200] Loss 0.7818, train acc 0.5978\n",
      "[Epoch 22/200] Loss 0.8239, train acc 0.5656\n",
      "[Epoch 23/200] Loss 0.7929, train acc 0.5878\n",
      "[Epoch 24/200] Loss 0.8018, train acc 0.5779\n",
      "[Epoch 25/200] Loss 0.7534, train acc 0.5995\n",
      "[Epoch 26/200] Loss 0.7778, train acc 0.5785\n",
      "[Epoch 27/200] Loss 0.7673, train acc 0.5989\n",
      "[Epoch 28/200] Loss 0.7795, train acc 0.5925\n",
      "[Epoch 29/200] Loss 0.7634, train acc 0.5919\n",
      "Accuracy: 0.8120\n",
      "[Epoch 30/200] Loss 0.7528, train acc 0.5884\n",
      "[Epoch 31/200] Loss 0.7227, train acc 0.6071\n",
      "[Epoch 32/200] Loss 0.7451, train acc 0.5972\n",
      "[Epoch 33/200] Loss 0.7282, train acc 0.5989\n",
      "[Epoch 34/200] Loss 0.7438, train acc 0.5972\n",
      "[Epoch 35/200] Loss 0.7316, train acc 0.5826\n",
      "[Epoch 36/200] Loss 0.7222, train acc 0.5972\n",
      "[Epoch 37/200] Loss 0.7249, train acc 0.6077\n",
      "[Epoch 38/200] Loss 0.7478, train acc 0.5919\n",
      "[Epoch 39/200] Loss 0.7115, train acc 0.6189\n",
      "Accuracy: 0.7820\n",
      "[Epoch 40/200] Loss 0.7484, train acc 0.5919\n",
      "[Epoch 41/200] Loss 0.7476, train acc 0.5896\n",
      "[Epoch 42/200] Loss 0.7536, train acc 0.5884\n",
      "[Epoch 43/200] Loss 0.7292, train acc 0.5954\n",
      "[Epoch 44/200] Loss 0.7558, train acc 0.5913\n",
      "[Epoch 45/200] Loss 0.7236, train acc 0.6083\n",
      "[Epoch 46/200] Loss 0.7289, train acc 0.5931\n",
      "[Epoch 47/200] Loss 0.7356, train acc 0.5937\n",
      "[Epoch 48/200] Loss 0.7324, train acc 0.6089\n",
      "[Epoch 49/200] Loss 0.7145, train acc 0.6077\n",
      "Accuracy: 0.7940\n",
      "[Epoch 50/200] Loss 0.7055, train acc 0.6224\n",
      "[Epoch 51/200] Loss 0.7545, train acc 0.5749\n",
      "[Epoch 52/200] Loss 0.7471, train acc 0.5855\n",
      "[Epoch 53/200] Loss 0.7207, train acc 0.6077\n",
      "[Epoch 54/200] Loss 0.7189, train acc 0.6013\n",
      "[Epoch 55/200] Loss 0.7255, train acc 0.6001\n",
      "[Epoch 56/200] Loss 0.7270, train acc 0.5919\n",
      "[Epoch 57/200] Loss 0.7316, train acc 0.6036\n",
      "[Epoch 58/200] Loss 0.7655, train acc 0.5726\n",
      "[Epoch 59/200] Loss 0.7447, train acc 0.5972\n",
      "Accuracy: 0.7860\n",
      "[Epoch 60/200] Loss 0.7672, train acc 0.5849\n",
      "[Epoch 61/200] Loss 0.7151, train acc 0.6066\n",
      "[Epoch 62/200] Loss 0.7478, train acc 0.5960\n",
      "[Epoch 63/200] Loss 0.7232, train acc 0.6030\n",
      "[Epoch 64/200] Loss 0.7696, train acc 0.5820\n",
      "[Epoch 65/200] Loss 0.7274, train acc 0.5972\n",
      "[Epoch 66/200] Loss 0.7287, train acc 0.5884\n",
      "[Epoch 67/200] Loss 0.7307, train acc 0.5855\n",
      "[Epoch 68/200] Loss 0.7525, train acc 0.5907\n",
      "[Epoch 69/200] Loss 0.7397, train acc 0.5931\n",
      "Accuracy: 0.7740\n",
      "[Epoch 70/200] Loss 0.7451, train acc 0.5779\n",
      "[Epoch 71/200] Loss 0.7226, train acc 0.6036\n",
      "[Epoch 72/200] Loss 0.7172, train acc 0.5925\n",
      "[Epoch 73/200] Loss 0.7252, train acc 0.6066\n",
      "[Epoch 74/200] Loss 0.7144, train acc 0.6025\n",
      "[Epoch 75/200] Loss 0.7403, train acc 0.5890\n",
      "[Epoch 76/200] Loss 0.7014, train acc 0.6142\n",
      "[Epoch 77/200] Loss 0.7423, train acc 0.5837\n",
      "[Epoch 78/200] Loss 0.7238, train acc 0.5948\n",
      "[Epoch 79/200] Loss 0.7349, train acc 0.5907\n",
      "Accuracy: 0.7760\n",
      "[Epoch 80/200] Loss 0.7241, train acc 0.5937\n",
      "[Epoch 81/200] Loss 0.7327, train acc 0.5948\n",
      "[Epoch 82/200] Loss 0.6900, train acc 0.6083\n",
      "[Epoch 83/200] Loss 0.7071, train acc 0.6136\n",
      "[Epoch 84/200] Loss 0.7131, train acc 0.6112\n",
      "[Epoch 85/200] Loss 0.7157, train acc 0.5896\n",
      "[Epoch 86/200] Loss 0.7198, train acc 0.5966\n",
      "[Epoch 87/200] Loss 0.6992, train acc 0.6177\n",
      "[Epoch 88/200] Loss 0.7292, train acc 0.6001\n",
      "[Epoch 89/200] Loss 0.7123, train acc 0.5948\n",
      "Accuracy: 0.7720\n",
      "[Epoch 90/200] Loss 0.7330, train acc 0.5925\n",
      "[Epoch 91/200] Loss 0.7182, train acc 0.6136\n",
      "[Epoch 92/200] Loss 0.7344, train acc 0.5902\n",
      "[Epoch 93/200] Loss 0.7268, train acc 0.5896\n",
      "[Epoch 94/200] Loss 0.7173, train acc 0.5978\n",
      "[Epoch 95/200] Loss 0.7112, train acc 0.6013\n",
      "[Epoch 96/200] Loss 0.7367, train acc 0.5913\n",
      "[Epoch 97/200] Loss 0.7200, train acc 0.5948\n",
      "[Epoch 98/200] Loss 0.7316, train acc 0.5878\n",
      "[Epoch 99/200] Loss 0.7390, train acc 0.5878\n",
      "Accuracy: 0.7820\n",
      "[Epoch 100/200] Loss 0.7277, train acc 0.5984\n",
      "[Epoch 101/200] Loss 0.7370, train acc 0.5919\n",
      "[Epoch 102/200] Loss 0.7165, train acc 0.6030\n",
      "[Epoch 103/200] Loss 0.7166, train acc 0.5966\n",
      "[Epoch 104/200] Loss 0.7145, train acc 0.6001\n",
      "[Epoch 105/200] Loss 0.7210, train acc 0.5954\n",
      "[Epoch 106/200] Loss 0.7248, train acc 0.6019\n",
      "[Epoch 107/200] Loss 0.7046, train acc 0.6124\n",
      "[Epoch 108/200] Loss 0.7042, train acc 0.6101\n",
      "[Epoch 109/200] Loss 0.6959, train acc 0.6183\n",
      "Accuracy: 0.7720\n",
      "[Epoch 110/200] Loss 0.7140, train acc 0.5989\n",
      "[Epoch 111/200] Loss 0.7401, train acc 0.5948\n",
      "[Epoch 112/200] Loss 0.7326, train acc 0.5925\n",
      "[Epoch 113/200] Loss 0.7187, train acc 0.6071\n",
      "[Epoch 114/200] Loss 0.7293, train acc 0.5773\n",
      "[Epoch 115/200] Loss 0.6806, train acc 0.6253\n",
      "[Epoch 116/200] Loss 0.7037, train acc 0.6159\n",
      "[Epoch 117/200] Loss 0.6944, train acc 0.6089\n",
      "[Epoch 118/200] Loss 0.6671, train acc 0.6276\n",
      "[Epoch 119/200] Loss 0.7015, train acc 0.5978\n",
      "Accuracy: 0.7580\n",
      "[Epoch 120/200] Loss 0.7358, train acc 0.5943\n",
      "[Epoch 121/200] Loss 0.7302, train acc 0.5820\n",
      "[Epoch 122/200] Loss 0.7176, train acc 0.6107\n",
      "[Epoch 123/200] Loss 0.7500, train acc 0.5779\n",
      "[Epoch 124/200] Loss 0.7017, train acc 0.6118\n",
      "[Epoch 125/200] Loss 0.7135, train acc 0.6089\n",
      "[Epoch 126/200] Loss 0.7336, train acc 0.5907\n",
      "[Epoch 127/200] Loss 0.7224, train acc 0.6095\n",
      "[Epoch 128/200] Loss 0.7056, train acc 0.5966\n",
      "[Epoch 129/200] Loss 0.6926, train acc 0.6171\n",
      "Accuracy: 0.7840\n",
      "[Epoch 130/200] Loss 0.7167, train acc 0.5890\n",
      "[Epoch 131/200] Loss 0.6989, train acc 0.6013\n",
      "[Epoch 132/200] Loss 0.6845, train acc 0.6200\n",
      "[Epoch 133/200] Loss 0.7172, train acc 0.5884\n",
      "[Epoch 134/200] Loss 0.7051, train acc 0.6089\n",
      "[Epoch 135/200] Loss 0.6828, train acc 0.6259\n",
      "[Epoch 136/200] Loss 0.6756, train acc 0.6194\n",
      "[Epoch 137/200] Loss 0.6920, train acc 0.6148\n",
      "[Epoch 138/200] Loss 0.6859, train acc 0.6189\n",
      "[Epoch 139/200] Loss 0.7272, train acc 0.5984\n",
      "Accuracy: 0.7760\n",
      "[Epoch 140/200] Loss 0.6896, train acc 0.6089\n",
      "[Epoch 141/200] Loss 0.7131, train acc 0.5960\n",
      "[Epoch 142/200] Loss 0.6948, train acc 0.6071\n",
      "[Epoch 143/200] Loss 0.7056, train acc 0.6013\n",
      "[Epoch 144/200] Loss 0.7197, train acc 0.5937\n",
      "[Epoch 145/200] Loss 0.7158, train acc 0.6001\n",
      "[Epoch 146/200] Loss 0.6775, train acc 0.6118\n",
      "[Epoch 147/200] Loss 0.7223, train acc 0.5902\n",
      "[Epoch 148/200] Loss 0.7078, train acc 0.6036\n",
      "[Epoch 149/200] Loss 0.7052, train acc 0.5989\n",
      "Accuracy: 0.7660\n",
      "[Epoch 150/200] Loss 0.7182, train acc 0.5925\n",
      "[Epoch 151/200] Loss 0.7434, train acc 0.5843\n",
      "[Epoch 152/200] Loss 0.6942, train acc 0.6148\n",
      "[Epoch 153/200] Loss 0.6932, train acc 0.6200\n",
      "[Epoch 154/200] Loss 0.7329, train acc 0.5878\n",
      "[Epoch 155/200] Loss 0.6904, train acc 0.6136\n",
      "[Epoch 156/200] Loss 0.7134, train acc 0.6066\n",
      "[Epoch 157/200] Loss 0.6607, train acc 0.6323\n",
      "[Epoch 158/200] Loss 0.6627, train acc 0.6323\n",
      "[Epoch 159/200] Loss 0.6839, train acc 0.6089\n",
      "Accuracy: 0.7720\n",
      "[Epoch 160/200] Loss 0.7226, train acc 0.5890\n",
      "[Epoch 161/200] Loss 0.7242, train acc 0.5960\n",
      "[Epoch 162/200] Loss 0.6838, train acc 0.6200\n",
      "[Epoch 163/200] Loss 0.7155, train acc 0.5902\n",
      "[Epoch 164/200] Loss 0.6961, train acc 0.6048\n",
      "[Epoch 165/200] Loss 0.7294, train acc 0.5884\n",
      "[Epoch 166/200] Loss 0.6889, train acc 0.6001\n",
      "[Epoch 167/200] Loss 0.7130, train acc 0.5890\n",
      "[Epoch 168/200] Loss 0.7051, train acc 0.6036\n",
      "[Epoch 169/200] Loss 0.7091, train acc 0.6042\n",
      "Accuracy: 0.7620\n",
      "[Epoch 170/200] Loss 0.6686, train acc 0.6194\n",
      "[Epoch 171/200] Loss 0.7127, train acc 0.6007\n",
      "[Epoch 172/200] Loss 0.6761, train acc 0.6218\n",
      "[Epoch 173/200] Loss 0.6592, train acc 0.6276\n",
      "[Epoch 174/200] Loss 0.7302, train acc 0.5890\n",
      "[Epoch 175/200] Loss 0.7194, train acc 0.6007\n",
      "[Epoch 176/200] Loss 0.6977, train acc 0.6060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 177/200] Loss 0.6934, train acc 0.6112\n",
      "[Epoch 178/200] Loss 0.7046, train acc 0.6007\n",
      "[Epoch 179/200] Loss 0.6945, train acc 0.6095\n",
      "Accuracy: 0.7580\n",
      "[Epoch 180/200] Loss 0.7026, train acc 0.6095\n",
      "[Epoch 181/200] Loss 0.7180, train acc 0.6019\n",
      "[Epoch 182/200] Loss 0.6815, train acc 0.6224\n",
      "[Epoch 183/200] Loss 0.7105, train acc 0.5989\n",
      "[Epoch 184/200] Loss 0.7264, train acc 0.6030\n",
      "[Epoch 185/200] Loss 0.7055, train acc 0.6007\n",
      "[Epoch 186/200] Loss 0.7408, train acc 0.5878\n",
      "[Epoch 187/200] Loss 0.7108, train acc 0.6107\n",
      "[Epoch 188/200] Loss 0.6827, train acc 0.6194\n",
      "[Epoch 189/200] Loss 0.7049, train acc 0.6165\n",
      "Accuracy: 0.7540\n",
      "[Epoch 190/200] Loss 0.7441, train acc 0.5872\n",
      "[Epoch 191/200] Loss 0.7204, train acc 0.5984\n",
      "[Epoch 192/200] Loss 0.7010, train acc 0.6030\n",
      "[Epoch 193/200] Loss 0.7234, train acc 0.5913\n",
      "[Epoch 194/200] Loss 0.7306, train acc 0.5931\n",
      "[Epoch 195/200] Loss 0.6992, train acc 0.6247\n",
      "[Epoch 196/200] Loss 0.6740, train acc 0.6358\n",
      "[Epoch 197/200] Loss 0.7420, train acc 0.5919\n",
      "[Epoch 198/200] Loss 0.7420, train acc 0.5808\n",
      "[Epoch 199/200] Loss 0.7195, train acc 0.6007\n",
      "Accuracy: 0.7600\n"
     ]
    }
   ],
   "source": [
    "#split dataset\n",
    "train_mask = torch.zeros(node_num, dtype = torch.bool)\n",
    "train_mask[:node_num - 1000] = 1               #1700左右training\n",
    "\n",
    "val_mask = None                                \n",
    "test_mask = torch.zeros(node_num, dtype = torch.bool)\n",
    "test_mask[node_num - 500:] = 1                 # 500test\n",
    "\n",
    "model = GCN(node_num, feat_dim, stat_dim, num_class).to(device)\n",
    "\n",
    "#Adam是一种算法，可以百度了解\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01, weight_decay = 1e-3)\n",
    "feat_Matrix = feat_Matrix.to(device)\n",
    "matDAD = edgeToMat(cites, node_num).to(device)\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(matDAD, feat_Matrix)\n",
    "    \n",
    "    loss = F.nll_loss(out[train_mask], label_list[train_mask])\n",
    "    _, pred = out.max(dim=1)\n",
    "    \n",
    "    correct = float(pred[train_mask].eq(label_list[train_mask]).sum().item())\n",
    "    acc = correct / train_mask.sum().item()\n",
    "    print('[Epoch {}/200] Loss {:.4f}, train acc {:.4f}'.format(epoch, loss.cpu().detach().data.item(), acc))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        _, pred = model(matDAD, feat_Matrix).max(dim = 1)\n",
    "        correct = float(pred[test_mask].eq(label_list[test_mask]).sum().item())\n",
    "        acc = correct / test_mask.sum().item()\n",
    "        print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
