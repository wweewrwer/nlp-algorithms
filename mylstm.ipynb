{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 79170 sentences.\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "print(\"Reading CSV file...\")\n",
    "with open('reddit-comments-2015-08.csv', 'rt', encoding= 'utf-8') as f:\n",
    "    reader = csv.reader(f, skipinitialspace = True)\n",
    "    next(reader)\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print(\"Parsed %d sentences.\" % (len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65441 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Found %d unique words tokens.\" % len(list(word_freq.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'crank' and appeared 10 times.\n"
     ]
    }
   ],
   "source": [
    "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "print(\"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print(\"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START what are n't you understanding about this ? !\n",
      "[0, 51, 27, 16, 10, 858, 54, 25, 34, 69]\n",
      "\n",
      "y:\n",
      "what are n't you understanding about this ? ! SENTENCE_END\n",
      "[51, 27, 16, 10, 858, 54, 25, 34, 69, 1]\n"
     ]
    }
   ],
   "source": [
    "x_example, y_example = x_train[17], y_train[17]\n",
    "print(\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
    "print(\"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        self.UI = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.WI = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "def forward_propagation(self, x):\n",
    "    T = len(x)\n",
    "    c = np.zeros((T + 1, self.hidden_dim))\n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    i = np.zeros((T + 1, self.hidden_dim))\n",
    "    g = np.zeros((T + 1, self.hidden_dim))\n",
    "    #s[-1] = np.zeros(self.hidden_dim)#why?\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    for t in np.arange(T):\n",
    "        i[t] = sigmoid(self.UI[:,x[t]] + self.WI.dot(s[t-1]))\n",
    "        g[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        c[t] = i[t] * g[t] + c[t - 1]\n",
    "        s[t] = np.tanh(c[t])\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return o, s, i, g\n",
    "    \n",
    "RNNNumpy.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    o, s, i, g = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "\n",
    "RNNNumpy.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 8000)\n",
      "[[1.24539713e-04 1.24701267e-04 1.25512467e-04 ... 1.25073936e-04\n",
      "  1.24941791e-04 1.25038483e-04]\n",
      " [1.24728275e-04 1.25025590e-04 1.25237914e-04 ... 1.25012753e-04\n",
      "  1.24725913e-04 1.24811728e-04]\n",
      " [1.24189293e-04 1.25211354e-04 1.25117430e-04 ... 1.25051213e-04\n",
      "  1.25136243e-04 1.25009317e-04]\n",
      " ...\n",
      " [1.84180924e-04 7.19536676e-05 1.55212077e-04 ... 1.80797232e-04\n",
      "  1.55387021e-04 5.96583727e-05]\n",
      " [1.81279989e-04 7.15597959e-05 1.48866275e-04 ... 1.81959775e-04\n",
      "  1.53312978e-04 5.70452747e-05]\n",
      " [1.78096379e-04 7.14925325e-05 1.42441645e-04 ... 1.82735063e-04\n",
      "  1.52071084e-04 5.48516533e-05]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s, i, g = model.forward_propagation(x_train[10])\n",
    "print(o.shape)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,)\n",
      "[1284 5674 5314 5314 5314 5314 2933 2933 2933 2933 6749 6749 6749 6749\n",
      " 6749 6749 6749 6749  881  881  881 3614 3614 3614 3614 3614 3614 3614\n",
      " 3614 3614 3614 3614 3614 3614 3614 4401 4401 4401 4401 4401 4401 4401\n",
      " 6528 6528 6528]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_train[10])\n",
    "print(predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    \n",
    "    for ii in np.arange(len(y)):\n",
    "        o, s, i, g = self.forward_propagation(x[ii])\n",
    "        correct_word_predictions = o[np.arange(len(y[ii])), y[ii]]\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(self, x, y):\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 8.987197\n",
      "Actual loss: 9.004405\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print(\"Actual loss: %f\" % model.calculate_loss(x_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s, i, g = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dtemp = np.zeros(s[0].shape)\n",
    "    dCdU = np.zeros(self.U.shape)\n",
    "    dCdW = np.zeros(self.W.shape)\n",
    "    dCdUI = np.zeros(self.UI.shape)\n",
    "    dCdWI = np.zeros(self.WI.shape)\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    dLdUI = np.zeros(self.UI.shape)\n",
    "    dLdWI = np.zeros(self.WI.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    for t in np.arange(T):\n",
    "        dtemp = i[t]*(1-g[t]**2)\n",
    "        dCdU[:,x[t]] += dtemp\n",
    "        dCdW += np.outer(dtemp, s[t - 1])\n",
    "        dtemp = g[t]*i[t]*(1-i[t])\n",
    "        dCdUI[:,x[t]] += dtemp\n",
    "        dCdW += np.outer(dtemp, s[t - 1])\n",
    "        \n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        dLdW += delta_t.dot(dCdW)\n",
    "        dLdWI += delta_t.dot(dCdWI)\n",
    "        dLdU += delta_t.dot(dCdU)\n",
    "        dLdUI += delta_t.dot(dCdUI)\n",
    "        \n",
    "        dtemp = i[t]*(1-g[t]**2)\n",
    "        dCdU[:,x[t]] -= dtemp\n",
    "        dCdW -= np.outer(dtemp, s[t - 1])\n",
    "        dtemp = g[t]*i[t]*(1-i[t])\n",
    "        dCdUI[:,x[t]] -= dtemp\n",
    "        dCdW -= np.outer(dtemp, s[t - 1])\n",
    "    return [dLdU, dLdV, dLdW, dLdUI, dLdWI]\n",
    "\n",
    "RNNNumpy.lstm = lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    dLdU, dLdV, dLdW, dLdUI, dLdWI =self.lstm(x, y)\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "    self.UI -= learning_rate * dLdUI\n",
    "    self.WI -= learning_rate * dLdWI\n",
    "    \n",
    "RNNNumpy.sgd_step = numpy_sdg_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_sgd(model, x_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        if(epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(x_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "            if(len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5\n",
    "                print(\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        for i in range(len(y_train)):\n",
    "            model.sgd_step(x_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307 ms ± 7.74 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "get_ipython().run_line_magic('timeit', 'model.sgd_step(x_train[10], y_train[10], 0.005)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-05 16:31:59: Loss after num_examples_seen=0 epoch=0: 9.003631\n",
      "2020-05-05 16:32:14: Loss after num_examples_seen=100 epoch=1: 6.600669\n",
      "2020-05-05 16:32:30: Loss after num_examples_seen=200 epoch=2: 6.159338\n",
      "2020-05-05 16:32:45: Loss after num_examples_seen=300 epoch=3: 5.954526\n",
      "2020-05-05 16:33:01: Loss after num_examples_seen=400 epoch=4: 5.837302\n",
      "2020-05-05 16:33:16: Loss after num_examples_seen=500 epoch=5: 5.758226\n",
      "2020-05-05 16:33:32: Loss after num_examples_seen=600 epoch=6: 5.700548\n",
      "2020-05-05 16:33:47: Loss after num_examples_seen=700 epoch=7: 5.656852\n",
      "2020-05-05 16:34:03: Loss after num_examples_seen=800 epoch=8: 5.624381\n",
      "2020-05-05 16:34:18: Loss after num_examples_seen=900 epoch=9: 5.597693\n",
      "2020-05-05 16:34:34: Loss after num_examples_seen=1000 epoch=10: 5.576755\n",
      "2020-05-05 16:34:50: Loss after num_examples_seen=1100 epoch=11: 5.560039\n",
      "2020-05-05 16:35:06: Loss after num_examples_seen=1200 epoch=12: 5.548914\n",
      "2020-05-05 16:35:22: Loss after num_examples_seen=1300 epoch=13: 5.538496\n",
      "2020-05-05 16:35:38: Loss after num_examples_seen=1400 epoch=14: 5.529899\n",
      "2020-05-05 16:35:54: Loss after num_examples_seen=1500 epoch=15: 5.522717\n",
      "2020-05-05 16:36:10: Loss after num_examples_seen=1600 epoch=16: 5.516654\n",
      "2020-05-05 16:36:25: Loss after num_examples_seen=1700 epoch=17: 5.511482\n",
      "2020-05-05 16:36:41: Loss after num_examples_seen=1800 epoch=18: 5.507026\n",
      "2020-05-05 16:36:57: Loss after num_examples_seen=1900 epoch=19: 5.503151\n",
      "2020-05-05 16:37:13: Loss after num_examples_seen=2000 epoch=20: 5.499755\n",
      "2020-05-05 16:37:29: Loss after num_examples_seen=2100 epoch=21: 5.496754\n",
      "2020-05-05 16:37:45: Loss after num_examples_seen=2200 epoch=22: 5.494086\n",
      "2020-05-05 16:38:00: Loss after num_examples_seen=2300 epoch=23: 5.491697\n",
      "2020-05-05 16:38:16: Loss after num_examples_seen=2400 epoch=24: 5.489547\n",
      "2020-05-05 16:38:32: Loss after num_examples_seen=2500 epoch=25: 5.487600\n",
      "2020-05-05 16:38:48: Loss after num_examples_seen=2600 epoch=26: 5.485830\n",
      "2020-05-05 16:39:03: Loss after num_examples_seen=2700 epoch=27: 5.484213\n",
      "2020-05-05 16:39:19: Loss after num_examples_seen=2800 epoch=28: 5.482729\n",
      "2020-05-05 16:39:34: Loss after num_examples_seen=2900 epoch=29: 5.481363\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = train_with_sgd(model, x_train[:100], y_train[:100], nepoch=30, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-05 17:06:25: Loss after num_examples_seen=0 epoch=0: 1.649836\n",
      "2020-05-05 17:06:33: Loss after num_examples_seen=100 epoch=100: 1.640159\n",
      "2020-05-05 17:06:42: Loss after num_examples_seen=200 epoch=200: 1.631072\n",
      "2020-05-05 17:06:50: Loss after num_examples_seen=300 epoch=300: 1.622525\n",
      "2020-05-05 17:06:58: Loss after num_examples_seen=400 epoch=400: 1.614474\n",
      "2020-05-05 17:07:07: Loss after num_examples_seen=500 epoch=500: 1.606879\n",
      "2020-05-05 17:07:15: Loss after num_examples_seen=600 epoch=600: 1.599703\n",
      "2020-05-05 17:07:24: Loss after num_examples_seen=700 epoch=700: 1.592915\n",
      "2020-05-05 17:07:32: Loss after num_examples_seen=800 epoch=800: 1.586485\n",
      "2020-05-05 17:07:41: Loss after num_examples_seen=900 epoch=900: 1.580386\n",
      "2020-05-05 17:07:50: Loss after num_examples_seen=1000 epoch=1000: 1.574594\n",
      "2020-05-05 17:07:58: Loss after num_examples_seen=1100 epoch=1100: 1.569084\n",
      "2020-05-05 17:08:07: Loss after num_examples_seen=1200 epoch=1200: 1.563838\n",
      "2020-05-05 17:08:15: Loss after num_examples_seen=1300 epoch=1300: 1.558836\n",
      "2020-05-05 17:08:24: Loss after num_examples_seen=1400 epoch=1400: 1.554060\n",
      "2020-05-05 17:08:32: Loss after num_examples_seen=1500 epoch=1500: 1.549494\n",
      "2020-05-05 17:08:41: Loss after num_examples_seen=1600 epoch=1600: 1.545124\n",
      "2020-05-05 17:08:49: Loss after num_examples_seen=1700 epoch=1700: 1.540936\n",
      "2020-05-05 17:08:58: Loss after num_examples_seen=1800 epoch=1800: 1.536918\n",
      "2020-05-05 17:09:06: Loss after num_examples_seen=1900 epoch=1900: 1.533057\n",
      "2020-05-05 17:09:15: Loss after num_examples_seen=2000 epoch=2000: 1.529345\n",
      "2020-05-05 17:09:24: Loss after num_examples_seen=2100 epoch=2100: 1.525769\n",
      "2020-05-05 17:09:32: Loss after num_examples_seen=2200 epoch=2200: 1.522323\n",
      "2020-05-05 17:09:41: Loss after num_examples_seen=2300 epoch=2300: 1.518997\n",
      "2020-05-05 17:09:51: Loss after num_examples_seen=2400 epoch=2400: 1.515783\n",
      "2020-05-05 17:09:59: Loss after num_examples_seen=2500 epoch=2500: 1.512675\n",
      "2020-05-05 17:10:08: Loss after num_examples_seen=2600 epoch=2600: 1.509666\n",
      "2020-05-05 17:10:17: Loss after num_examples_seen=2700 epoch=2700: 1.506750\n",
      "2020-05-05 17:10:26: Loss after num_examples_seen=2800 epoch=2800: 1.503921\n",
      "2020-05-05 17:10:35: Loss after num_examples_seen=2900 epoch=2900: 1.501174\n",
      "2020-05-05 17:10:43: Loss after num_examples_seen=3000 epoch=3000: 1.498503\n",
      "2020-05-05 17:10:52: Loss after num_examples_seen=3100 epoch=3100: 1.495905\n",
      "2020-05-05 17:11:01: Loss after num_examples_seen=3200 epoch=3200: 1.493375\n",
      "2020-05-05 17:11:09: Loss after num_examples_seen=3300 epoch=3300: 1.490909\n",
      "2020-05-05 17:11:18: Loss after num_examples_seen=3400 epoch=3400: 1.488503\n",
      "2020-05-05 17:11:26: Loss after num_examples_seen=3500 epoch=3500: 1.486154\n",
      "2020-05-05 17:11:35: Loss after num_examples_seen=3600 epoch=3600: 1.483858\n",
      "2020-05-05 17:11:43: Loss after num_examples_seen=3700 epoch=3700: 1.481613\n",
      "2020-05-05 17:11:52: Loss after num_examples_seen=3800 epoch=3800: 1.479415\n",
      "2020-05-05 17:12:01: Loss after num_examples_seen=3900 epoch=3900: 1.477262\n",
      "2020-05-05 17:12:10: Loss after num_examples_seen=4000 epoch=4000: 1.475152\n",
      "2020-05-05 17:12:18: Loss after num_examples_seen=4100 epoch=4100: 1.473081\n",
      "2020-05-05 17:12:27: Loss after num_examples_seen=4200 epoch=4200: 1.471049\n",
      "2020-05-05 17:12:35: Loss after num_examples_seen=4300 epoch=4300: 1.469051\n",
      "2020-05-05 17:12:44: Loss after num_examples_seen=4400 epoch=4400: 1.467088\n",
      "2020-05-05 17:12:52: Loss after num_examples_seen=4500 epoch=4500: 1.465156\n",
      "2020-05-05 17:13:01: Loss after num_examples_seen=4600 epoch=4600: 1.463255\n",
      "2020-05-05 17:13:09: Loss after num_examples_seen=4700 epoch=4700: 1.461381\n",
      "2020-05-05 17:13:18: Loss after num_examples_seen=4800 epoch=4800: 1.459535\n",
      "2020-05-05 17:13:27: Loss after num_examples_seen=4900 epoch=4900: 1.457714\n",
      "2020-05-05 17:13:36: Loss after num_examples_seen=5000 epoch=5000: 1.455917\n",
      "2020-05-05 17:13:44: Loss after num_examples_seen=5100 epoch=5100: 1.454143\n",
      "2020-05-05 17:13:53: Loss after num_examples_seen=5200 epoch=5200: 1.452390\n",
      "2020-05-05 17:14:02: Loss after num_examples_seen=5300 epoch=5300: 1.450657\n",
      "2020-05-05 17:14:10: Loss after num_examples_seen=5400 epoch=5400: 1.448943\n",
      "2020-05-05 17:14:20: Loss after num_examples_seen=5500 epoch=5500: 1.447248\n",
      "2020-05-05 17:14:28: Loss after num_examples_seen=5600 epoch=5600: 1.445570\n",
      "2020-05-05 17:14:37: Loss after num_examples_seen=5700 epoch=5700: 1.443907\n",
      "2020-05-05 17:14:46: Loss after num_examples_seen=5800 epoch=5800: 1.442260\n",
      "2020-05-05 17:14:55: Loss after num_examples_seen=5900 epoch=5900: 1.440628\n",
      "2020-05-05 17:15:03: Loss after num_examples_seen=6000 epoch=6000: 1.439008\n",
      "2020-05-05 17:15:12: Loss after num_examples_seen=6100 epoch=6100: 1.437402\n",
      "2020-05-05 17:15:21: Loss after num_examples_seen=6200 epoch=6200: 1.435807\n",
      "2020-05-05 17:15:30: Loss after num_examples_seen=6300 epoch=6300: 1.434223\n",
      "2020-05-05 17:15:38: Loss after num_examples_seen=6400 epoch=6400: 1.432650\n",
      "2020-05-05 17:15:47: Loss after num_examples_seen=6500 epoch=6500: 1.431086\n",
      "2020-05-05 17:15:55: Loss after num_examples_seen=6600 epoch=6600: 1.429531\n",
      "2020-05-05 17:16:04: Loss after num_examples_seen=6700 epoch=6700: 1.427984\n",
      "2020-05-05 17:16:13: Loss after num_examples_seen=6800 epoch=6800: 1.426444\n",
      "2020-05-05 17:16:21: Loss after num_examples_seen=6900 epoch=6900: 1.424911\n",
      "2020-05-05 17:16:30: Loss after num_examples_seen=7000 epoch=7000: 1.423384\n",
      "2020-05-05 17:16:39: Loss after num_examples_seen=7100 epoch=7100: 1.421862\n",
      "2020-05-05 17:16:47: Loss after num_examples_seen=7200 epoch=7200: 1.420345\n",
      "2020-05-05 17:16:56: Loss after num_examples_seen=7300 epoch=7300: 1.418832\n",
      "2020-05-05 17:17:05: Loss after num_examples_seen=7400 epoch=7400: 1.417322\n",
      "2020-05-05 17:17:13: Loss after num_examples_seen=7500 epoch=7500: 1.415815\n",
      "2020-05-05 17:17:22: Loss after num_examples_seen=7600 epoch=7600: 1.414310\n",
      "2020-05-05 17:17:30: Loss after num_examples_seen=7700 epoch=7700: 1.412806\n",
      "2020-05-05 17:17:39: Loss after num_examples_seen=7800 epoch=7800: 1.411304\n",
      "2020-05-05 17:17:48: Loss after num_examples_seen=7900 epoch=7900: 1.409802\n",
      "2020-05-05 17:17:56: Loss after num_examples_seen=8000 epoch=8000: 1.408299\n",
      "2020-05-05 17:18:05: Loss after num_examples_seen=8100 epoch=8100: 1.406797\n",
      "2020-05-05 17:18:14: Loss after num_examples_seen=8200 epoch=8200: 1.405293\n",
      "2020-05-05 17:18:23: Loss after num_examples_seen=8300 epoch=8300: 1.403788\n",
      "2020-05-05 17:18:32: Loss after num_examples_seen=8400 epoch=8400: 1.402281\n",
      "2020-05-05 17:18:40: Loss after num_examples_seen=8500 epoch=8500: 1.400772\n",
      "2020-05-05 17:18:49: Loss after num_examples_seen=8600 epoch=8600: 1.399261\n",
      "2020-05-05 17:18:58: Loss after num_examples_seen=8700 epoch=8700: 1.397748\n",
      "2020-05-05 17:19:06: Loss after num_examples_seen=8800 epoch=8800: 1.396232\n",
      "2020-05-05 17:19:15: Loss after num_examples_seen=8900 epoch=8900: 1.394714\n",
      "2020-05-05 17:19:23: Loss after num_examples_seen=9000 epoch=9000: 1.393194\n",
      "2020-05-05 17:19:32: Loss after num_examples_seen=9100 epoch=9100: 1.391671\n",
      "2020-05-05 17:19:40: Loss after num_examples_seen=9200 epoch=9200: 1.390146\n",
      "2020-05-05 17:19:49: Loss after num_examples_seen=9300 epoch=9300: 1.388618\n",
      "2020-05-05 17:19:58: Loss after num_examples_seen=9400 epoch=9400: 1.387089\n",
      "2020-05-05 17:20:06: Loss after num_examples_seen=9500 epoch=9500: 1.385558\n",
      "2020-05-05 17:20:15: Loss after num_examples_seen=9600 epoch=9600: 1.384025\n",
      "2020-05-05 17:20:23: Loss after num_examples_seen=9700 epoch=9700: 1.382492\n",
      "2020-05-05 17:20:32: Loss after num_examples_seen=9800 epoch=9800: 1.380957\n",
      "2020-05-05 17:20:40: Loss after num_examples_seen=9900 epoch=9900: 1.379421\n"
     ]
    }
   ],
   "source": [
    "losses = train_with_sgd(model, x_train[1:2], y_train[1:2], nepoch=10000, evaluate_loss_after=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)[0]\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it ppr ppr . ppr . ppr . .\n",
      "math ppr . . . . ppr\n",
      ". . ppr ppr .\n",
      ". ppr a . ppr\n",
      "a ppr ppr . 's ppr\n",
      "it 's 's slight ppr\n",
      "it 's slight a ppr ppr\n",
      "'s a slight a ppr\n",
      ". ppr it ppr . ppr . . .\n",
      "ppr 's ppr . ppr\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 10\n",
    "senten_min_length = 5\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print(\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
