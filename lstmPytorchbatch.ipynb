{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 79170 sentences.\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 4000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "sentence_extra = \"SENTENCE_EXTRA\"\n",
    "max_len = 30\n",
    "\n",
    "print(\"Reading CSV file...\")\n",
    "with open('reddit-comments-2015-08.csv', 'rt', encoding= 'utf-8') as f:\n",
    "    reader = csv.reader(f, skipinitialspace = True)\n",
    "    next(reader)\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print(\"Parsed %d sentences.\" % (len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 64109 sentences with len 30.\n"
     ]
    }
   ],
   "source": [
    "max_len = 30\n",
    "ts = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "tokenized_sentences = []\n",
    "end_token = [sentence_extra]*40\n",
    "for x in ts:\n",
    "    if len(x) > max_len:\n",
    "        continue\n",
    "    tokenized_sentences.append(x + end_token[:max_len-len(x)])\n",
    "print(\"Parsed %d sentences with len %d.\" % (len(tokenized_sentences),len(tokenized_sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45260 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Found %d unique words tokens.\" % len(list(word_freq.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 4000.\n",
      "The least frequent word in our vocabulary is 'impression' and appeared 14 times.\n"
     ]
    }
   ],
   "source": [
    "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'UNKNOWN_TOKEN', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END', 'SENTENCE_EXTRA', 'SENTENCE_EXTRA', 'SENTENCE_EXTRA', 'SENTENCE_EXTRA', 'SENTENCE_EXTRA', 'SENTENCE_EXTRA', 'SENTENCE_EXTRA', 'SENTENCE_EXTRA', 'SENTENCE_EXTRA']'\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "print(\"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print(\"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START i 'd like to make it available , but there 's no point in making it mandatory . SENTENCE_END SENTENCE_EXTRA SENTENCE_EXTRA SENTENCE_EXTRA SENTENCE_EXTRA SENTENCE_EXTRA SENTENCE_EXTRA SENTENCE_EXTRA SENTENCE_EXTRA SENTENCE_EXTRA\n",
      "[   1    6  144   41    7  101   11  724    5   25   48   17   66  174\n",
      "   15  290   11 2653    3    2    0    0    0    0    0    0    0    0\n",
      "    0]\n",
      "\n",
      "y:\n",
      "i 'd like to make it available , but there 's no point in making it mandatory . SENTENCE_END SENTENCE_EXTRA SENTENCE_EXTRA SENTENCE_EXTRA SENTENCE_EXTRA SENTENCE_EXTRA SENTENCE_EXTRA SENTENCE_EXTRA SENTENCE_EXTRA SENTENCE_EXTRA SENTENCE_EXTRA\n",
      "[   6  144   41    7  101   11  724    5   25   48   17   66  174   15\n",
      "  290   11 2653    3    2    0    0    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "x_example, y_example = x_train[17], y_train[17]\n",
    "print(\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
    "print(\"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUpytorch(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim=100, bptt_truncate=4, batch_size=3):\n",
    "        super().__init__()\n",
    "        self.word_dim = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        self.batch_size = 3\n",
    "        \n",
    "        self.gru = nn.GRU(vocab_size, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim,vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_train = F.one_hot(x, num_classes=self.word_dim)\n",
    "        ht,_ = self.gru(x_train.float())\n",
    "        st = self.linear(ht.contiguous().view(ht.shape[0]*ht.shape[1],self.hidden_dim))\n",
    "        yt = F.log_softmax(st, dim=1)\n",
    "        return yt\n",
    "    def pre(self, x):\n",
    "        x_train = F.one_hot(x, num_classes=self.word_dim)\n",
    "        ht,_ = self.gru(x_train.view(1,len(x_train),-1).float())\n",
    "        st = self.linear(ht.view(len(x_train),-1))\n",
    "        yt = F.log_softmax(st, dim=1)\n",
    "        return yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "def train_with_sgd(model, x_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5,batch_size=64):\n",
    "    num_examples_seen = 0\n",
    "    calculate_loss=nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.005)\n",
    "    N = np.sum((len(y_i) for y_i in y_train))\n",
    "    train_ds = TensorDataset(x_train,y_train)\n",
    "    for epoch in range(nepoch):\n",
    "        loss_total=0\n",
    "        for i in range(batch_size-1,len(y_train),batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            xb,yb=train_ds[i-batch_size+1:i+1]\n",
    "            y_hat=model.forward(xb)\n",
    "            loss=calculate_loss(y_hat,yb.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            loss_total+=loss\n",
    "            optimizer.step()\n",
    "            \n",
    "            num_examples_seen += batch_size\n",
    "        if(epoch % evaluate_loss_after == 0):\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss_total/N))\n",
    "            sys.stdout.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 10:30:38: Loss after num_examples_seen=936 epoch=0: 0.004317\n",
      "2020-05-24 10:30:40: Loss after num_examples_seen=1872 epoch=1: 0.003885\n",
      "2020-05-24 10:30:42: Loss after num_examples_seen=2808 epoch=2: 0.002753\n",
      "2020-05-24 10:30:44: Loss after num_examples_seen=3744 epoch=3: 0.002460\n",
      "2020-05-24 10:30:47: Loss after num_examples_seen=4680 epoch=4: 0.002421\n",
      "2020-05-24 10:30:49: Loss after num_examples_seen=5616 epoch=5: 0.002386\n",
      "2020-05-24 10:30:51: Loss after num_examples_seen=6552 epoch=6: 0.002354\n",
      "2020-05-24 10:30:53: Loss after num_examples_seen=7488 epoch=7: 0.002322\n",
      "2020-05-24 10:30:55: Loss after num_examples_seen=8424 epoch=8: 0.002290\n",
      "2020-05-24 10:30:57: Loss after num_examples_seen=9360 epoch=9: 0.002258\n"
     ]
    }
   ],
   "source": [
    "model=GRUpytorch(vocabulary_size)\n",
    "#dev='cpu'\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(dev)\n",
    "xx = torch.tensor(x_train,device=dev).long()\n",
    "yy = torch.tensor(y_train,device=dev).long()\n",
    "losses = train_with_sgd(model, xx[:10000], yy[:10000], nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 10:31:28: Loss after num_examples_seen=9984 epoch=0: 0.002225\n",
      "2020-05-24 10:31:31: Loss after num_examples_seen=19968 epoch=1: 0.002194\n",
      "2020-05-24 10:31:33: Loss after num_examples_seen=29952 epoch=2: 0.002169\n",
      "2020-05-24 10:31:35: Loss after num_examples_seen=39936 epoch=3: 0.002148\n",
      "2020-05-24 10:31:37: Loss after num_examples_seen=49920 epoch=4: 0.002129\n",
      "2020-05-24 10:31:39: Loss after num_examples_seen=59904 epoch=5: 0.002113\n",
      "2020-05-24 10:31:41: Loss after num_examples_seen=69888 epoch=6: 0.002097\n",
      "2020-05-24 10:31:44: Loss after num_examples_seen=79872 epoch=7: 0.002082\n",
      "2020-05-24 10:31:46: Loss after num_examples_seen=89856 epoch=8: 0.002067\n",
      "2020-05-24 10:31:48: Loss after num_examples_seen=99840 epoch=9: 0.002052\n",
      "2020-05-24 10:31:50: Loss after num_examples_seen=109824 epoch=10: 0.002037\n",
      "2020-05-24 10:31:52: Loss after num_examples_seen=119808 epoch=11: 0.002022\n",
      "2020-05-24 10:31:54: Loss after num_examples_seen=129792 epoch=12: 0.002007\n",
      "2020-05-24 10:31:57: Loss after num_examples_seen=139776 epoch=13: 0.001993\n",
      "2020-05-24 10:31:59: Loss after num_examples_seen=149760 epoch=14: 0.001980\n",
      "2020-05-24 10:32:01: Loss after num_examples_seen=159744 epoch=15: 0.001966\n",
      "2020-05-24 10:32:03: Loss after num_examples_seen=169728 epoch=16: 0.001954\n",
      "2020-05-24 10:32:05: Loss after num_examples_seen=179712 epoch=17: 0.001942\n",
      "2020-05-24 10:32:08: Loss after num_examples_seen=189696 epoch=18: 0.001931\n",
      "2020-05-24 10:32:10: Loss after num_examples_seen=199680 epoch=19: 0.001921\n",
      "2020-05-24 10:32:12: Loss after num_examples_seen=209664 epoch=20: 0.001912\n",
      "2020-05-24 10:32:14: Loss after num_examples_seen=219648 epoch=21: 0.001902\n",
      "2020-05-24 10:32:16: Loss after num_examples_seen=229632 epoch=22: 0.001894\n",
      "2020-05-24 10:32:18: Loss after num_examples_seen=239616 epoch=23: 0.001886\n",
      "2020-05-24 10:32:21: Loss after num_examples_seen=249600 epoch=24: 0.001878\n",
      "2020-05-24 10:32:23: Loss after num_examples_seen=259584 epoch=25: 0.001871\n",
      "2020-05-24 10:32:25: Loss after num_examples_seen=269568 epoch=26: 0.001864\n",
      "2020-05-24 10:32:27: Loss after num_examples_seen=279552 epoch=27: 0.001857\n",
      "2020-05-24 10:32:29: Loss after num_examples_seen=289536 epoch=28: 0.001850\n",
      "2020-05-24 10:32:32: Loss after num_examples_seen=299520 epoch=29: 0.001844\n",
      "2020-05-24 10:32:34: Loss after num_examples_seen=309504 epoch=30: 0.001838\n",
      "2020-05-24 10:32:36: Loss after num_examples_seen=319488 epoch=31: 0.001832\n",
      "2020-05-24 10:32:38: Loss after num_examples_seen=329472 epoch=32: 0.001827\n",
      "2020-05-24 10:32:40: Loss after num_examples_seen=339456 epoch=33: 0.001821\n",
      "2020-05-24 10:32:43: Loss after num_examples_seen=349440 epoch=34: 0.001816\n",
      "2020-05-24 10:32:45: Loss after num_examples_seen=359424 epoch=35: 0.001811\n",
      "2020-05-24 10:32:47: Loss after num_examples_seen=369408 epoch=36: 0.001807\n",
      "2020-05-24 10:32:49: Loss after num_examples_seen=379392 epoch=37: 0.001802\n",
      "2020-05-24 10:32:51: Loss after num_examples_seen=389376 epoch=38: 0.001797\n",
      "2020-05-24 10:32:54: Loss after num_examples_seen=399360 epoch=39: 0.001793\n",
      "2020-05-24 10:32:56: Loss after num_examples_seen=409344 epoch=40: 0.001789\n",
      "2020-05-24 10:32:58: Loss after num_examples_seen=419328 epoch=41: 0.001784\n",
      "2020-05-24 10:33:00: Loss after num_examples_seen=429312 epoch=42: 0.001780\n",
      "2020-05-24 10:33:02: Loss after num_examples_seen=439296 epoch=43: 0.001776\n",
      "2020-05-24 10:33:05: Loss after num_examples_seen=449280 epoch=44: 0.001772\n",
      "2020-05-24 10:33:07: Loss after num_examples_seen=459264 epoch=45: 0.001769\n",
      "2020-05-24 10:33:09: Loss after num_examples_seen=469248 epoch=46: 0.001765\n",
      "2020-05-24 10:33:11: Loss after num_examples_seen=479232 epoch=47: 0.001761\n",
      "2020-05-24 10:33:13: Loss after num_examples_seen=489216 epoch=48: 0.001758\n",
      "2020-05-24 10:33:16: Loss after num_examples_seen=499200 epoch=49: 0.001754\n",
      "2020-05-24 10:33:18: Loss after num_examples_seen=509184 epoch=50: 0.001751\n",
      "2020-05-24 10:33:20: Loss after num_examples_seen=519168 epoch=51: 0.001748\n",
      "2020-05-24 10:33:22: Loss after num_examples_seen=529152 epoch=52: 0.001745\n",
      "2020-05-24 10:33:24: Loss after num_examples_seen=539136 epoch=53: 0.001742\n",
      "2020-05-24 10:33:27: Loss after num_examples_seen=549120 epoch=54: 0.001739\n",
      "2020-05-24 10:33:29: Loss after num_examples_seen=559104 epoch=55: 0.001736\n",
      "2020-05-24 10:33:31: Loss after num_examples_seen=569088 epoch=56: 0.001733\n",
      "2020-05-24 10:33:33: Loss after num_examples_seen=579072 epoch=57: 0.001730\n",
      "2020-05-24 10:33:35: Loss after num_examples_seen=589056 epoch=58: 0.001728\n",
      "2020-05-24 10:33:38: Loss after num_examples_seen=599040 epoch=59: 0.001725\n",
      "2020-05-24 10:33:40: Loss after num_examples_seen=609024 epoch=60: 0.001722\n",
      "2020-05-24 10:33:42: Loss after num_examples_seen=619008 epoch=61: 0.001720\n",
      "2020-05-24 10:33:44: Loss after num_examples_seen=628992 epoch=62: 0.001717\n",
      "2020-05-24 10:33:46: Loss after num_examples_seen=638976 epoch=63: 0.001715\n",
      "2020-05-24 10:33:49: Loss after num_examples_seen=648960 epoch=64: 0.001713\n",
      "2020-05-24 10:33:51: Loss after num_examples_seen=658944 epoch=65: 0.001710\n",
      "2020-05-24 10:33:53: Loss after num_examples_seen=668928 epoch=66: 0.001708\n",
      "2020-05-24 10:33:55: Loss after num_examples_seen=678912 epoch=67: 0.001706\n",
      "2020-05-24 10:33:57: Loss after num_examples_seen=688896 epoch=68: 0.001704\n",
      "2020-05-24 10:34:00: Loss after num_examples_seen=698880 epoch=69: 0.001702\n",
      "2020-05-24 10:34:02: Loss after num_examples_seen=708864 epoch=70: 0.001700\n",
      "2020-05-24 10:34:04: Loss after num_examples_seen=718848 epoch=71: 0.001698\n",
      "2020-05-24 10:34:06: Loss after num_examples_seen=728832 epoch=72: 0.001696\n",
      "2020-05-24 10:34:08: Loss after num_examples_seen=738816 epoch=73: 0.001694\n",
      "2020-05-24 10:34:11: Loss after num_examples_seen=748800 epoch=74: 0.001692\n",
      "2020-05-24 10:34:13: Loss after num_examples_seen=758784 epoch=75: 0.001690\n",
      "2020-05-24 10:34:15: Loss after num_examples_seen=768768 epoch=76: 0.001688\n",
      "2020-05-24 10:34:17: Loss after num_examples_seen=778752 epoch=77: 0.001686\n",
      "2020-05-24 10:34:20: Loss after num_examples_seen=788736 epoch=78: 0.001684\n",
      "2020-05-24 10:34:22: Loss after num_examples_seen=798720 epoch=79: 0.001683\n",
      "2020-05-24 10:34:24: Loss after num_examples_seen=808704 epoch=80: 0.001681\n",
      "2020-05-24 10:34:26: Loss after num_examples_seen=818688 epoch=81: 0.001679\n",
      "2020-05-24 10:34:28: Loss after num_examples_seen=828672 epoch=82: 0.001677\n",
      "2020-05-24 10:34:31: Loss after num_examples_seen=838656 epoch=83: 0.001676\n",
      "2020-05-24 10:34:33: Loss after num_examples_seen=848640 epoch=84: 0.001674\n",
      "2020-05-24 10:34:35: Loss after num_examples_seen=858624 epoch=85: 0.001672\n",
      "2020-05-24 10:34:37: Loss after num_examples_seen=868608 epoch=86: 0.001671\n",
      "2020-05-24 10:34:39: Loss after num_examples_seen=878592 epoch=87: 0.001669\n",
      "2020-05-24 10:34:42: Loss after num_examples_seen=888576 epoch=88: 0.001668\n",
      "2020-05-24 10:34:44: Loss after num_examples_seen=898560 epoch=89: 0.001666\n",
      "2020-05-24 10:34:46: Loss after num_examples_seen=908544 epoch=90: 0.001664\n",
      "2020-05-24 10:34:48: Loss after num_examples_seen=918528 epoch=91: 0.001663\n",
      "2020-05-24 10:34:50: Loss after num_examples_seen=928512 epoch=92: 0.001661\n",
      "2020-05-24 10:34:53: Loss after num_examples_seen=938496 epoch=93: 0.001660\n",
      "2020-05-24 10:34:55: Loss after num_examples_seen=948480 epoch=94: 0.001658\n",
      "2020-05-24 10:34:57: Loss after num_examples_seen=958464 epoch=95: 0.001657\n",
      "2020-05-24 10:34:59: Loss after num_examples_seen=968448 epoch=96: 0.001656\n",
      "2020-05-24 10:35:02: Loss after num_examples_seen=978432 epoch=97: 0.001654\n",
      "2020-05-24 10:35:04: Loss after num_examples_seen=988416 epoch=98: 0.001653\n",
      "2020-05-24 10:35:06: Loss after num_examples_seen=998400 epoch=99: 0.001651\n"
     ]
    }
   ],
   "source": [
    "#model.to(dev)\n",
    "losses = train_with_sgd(model, xx[:10000], yy[:10000], nepoch=100, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "  with torch.no_grad():\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Repeat until we get an end token\n",
    "    while (not new_sentence[-1] == word_to_index[sentence_end_token] and not new_sentence[-1] == word_to_index[sentence_extra]):\n",
    "        next_word_probs = np.exp(model.pre(torch.tensor(new_sentence).long()))\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "families israel and and , you a , the they , transfer billion , in for drag identify mode it n't 's and confidence sometimes the . but to i to to n't\n",
      "manufacturing her of is the that as to batteries that thin\n",
      "culture ugly examples the early dry you\n",
      "apps other heavily with purpose\n",
      "book it has confident is . in is poverty\n",
      "involving replied soft yay ago what with decides to your of of showing ? already to made is the 's with am of bonus for .\n",
      "normal ask a political enjoyable given ^^^have , any attacks on if do\n",
      "bowl they he spent fastest wash microsoft , that player a\n",
      "lacking system several statement 's accidentally town legs blade canada\n",
      "sexual chest was most ahead fan hotel riding that pour san a and you\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 10\n",
    "senten_min_length = 1\n",
    "model.to('cpu')\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print(\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
